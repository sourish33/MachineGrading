{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import os.path\n",
    "from os import remove\n",
    "import random\n",
    "from time import time\n",
    "import string\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from sys import exit\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "\n",
    "def text_between_subtrings(s, start, end):\n",
    "    return s.find(start), s.find(end)\n",
    "\n",
    "def wordify(question, max_words=30):\n",
    "    question_words=question.split()\n",
    "    if len(question_words)>max_words:\n",
    "        question_words=question_words[:max_words]\n",
    "    q=' '.join(question_words)\n",
    "    q=re.sub(' to power of ', '^', q)\n",
    "    q=re.sub('with subscript\\(\\(', '', q)\n",
    "    q=re.sub('\\)\\)', '', q)\n",
    "    q=re.sub('\\d+\\) _+', '', q)#these two lines get rid of any 3) _______ crap\n",
    "    q = re.sub('#\\)', '', q)\n",
    "    return q.strip()\n",
    "\n",
    "def wordify_answers(answer, max_words=30):\n",
    "    answer_words=answer.split()\n",
    "    if len(answer_words)>max_words:\n",
    "        answer_words=answer_words[:max_words]\n",
    "    q=' '.join(answer_words)\n",
    "    q=re.sub(' to power of ', '^', q)\n",
    "    if len(answer_words)>1:\n",
    "        q=re.sub('[\\bA-E]\\)', '', q)#strips the A) for all except the version question\n",
    "    return q.strip()\n",
    "\n",
    "def extract_questions(s, max_words=10):\n",
    "    more='yes'\n",
    "    n=1\n",
    "    questionlist=[]\n",
    "    while more=='yes':\n",
    "        start = '\\n'+str(n)\n",
    "        end = '\\n'+str(n+1)\n",
    "        p1, p2 = text_between_subtrings(s, start, end)\n",
    "        if p1!=-1 and p2!=-1:\n",
    "            txt=s[p1+len(start):p2]\n",
    "            ###Grab the part between ) and nA, and convert into words\n",
    "            pp1, pp2 = text_between_subtrings(txt, ')', '\\nA')\n",
    "            txt=txt[1:txt.find('\\nA')]\n",
    "            txt=wordify(txt,max_words)\n",
    "            questionlist.append(txt)\n",
    "            n+=1\n",
    "        elif p1!=-1 and p2==-1:\n",
    "            txt=s[p1+len(start):]\n",
    "            pp1, pp2 = text_between_subtrings(txt, ')', '\\nA')\n",
    "            txt=txt[1:txt.find('\\nA')]\n",
    "            txt=wordify(txt,max_words)\n",
    "            #re.sub('[^A-Z]', '', s)\n",
    "            questionlist.append(txt)\n",
    "            more='no'\n",
    "        else:\n",
    "            print(\"Sum ting wong. Shouldn't go here. p1= \"+str(p1)+\" p2= \"+str(p2))\n",
    "            more='no'\n",
    "    return questionlist\n",
    "    \n",
    "\n",
    "\n",
    "def createQuestionIDs_and_dfs(max_words=30):\n",
    "    All_IDs=[]\n",
    "    dfs=[]\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        if (os.path.isfile(filename)):\n",
    "            with open(filename, encoding=\"utf-8\") as file:  \n",
    "                data = file.read()\n",
    "            p=data.rfind('\\n\\n\\n\\n1)')\n",
    "            if p!=-1: \n",
    "                questions=data[:p]\n",
    "            else:\n",
    "                print('Make sure questions and answers in key'+str(n)+' are separated by \\n\\n\\n\\n')\n",
    "                return []\n",
    "            if questions.find('version is this')==-1:\n",
    "                print('Make sure there is a version question with the words \"version is this\". ')\n",
    "                return []\n",
    "            \n",
    "            qids=extract_questions(questions, max_words)\n",
    "            qnum=len(qids)\n",
    "            np_qids=np.array(qids).reshape(qnum,1)\n",
    "            All_IDs.append(qids)\n",
    "            \n",
    "            \n",
    "            aa=extract_choices(questions, qnum)\n",
    "            np_aa=np.array(aa).reshape(qnum,len(aa[0]))\n",
    "            v1=np.concatenate((np_qids,np_aa),axis=1)\n",
    "            df=pd.DataFrame(v1,columns=[\"Question\",\"A\",\"B\",\"C\",\"D\",\"E\"],index=range(1,qnum+1))\n",
    "            df['Blank']=''\n",
    "            dfs.append(df)\n",
    "     \n",
    "    '''This part takes All_IDs which stores the full text of the questions and assigns the numbers \n",
    "    0....N to the questions of the first version. These numbers then become identifiers for each question. \n",
    "    Example: version 0: [what, when, how] and version 1: [how, when, what]. these will become [0, 1, 2] and\n",
    "    [2, 1, 0] respectively'''\n",
    "    qids_new=[]\n",
    "    qid_list=[x for x in range(0, qnum)]\n",
    "    qids_new.append(qid_list)\n",
    "    print(\"Generating QIDs. This might take a while..\")\n",
    "    starttime = time()\n",
    "    for whichlist in All_IDs[1:]:\n",
    "        qid_list=[1]*len(All_IDs[0])\n",
    "        for m in range(0,len(All_IDs[0])):\n",
    "            matching_question=process.extractOne(All_IDs[0][m], whichlist)[0]\n",
    "            qid_list[whichlist.index(matching_question)]=m\n",
    "        qids_new.append(qid_list)\n",
    "    endtime = time()\n",
    "    \n",
    "    print(\"Done. That took: \"+str(endtime-starttime)+ \" sec.\")\n",
    "    return {\"IDs\":qids_new, \"dfs\":dfs}        \n",
    "\n",
    "def extract_choices(s, qnum):\n",
    "    all_choices=[]\n",
    "    for n in range(1,qnum+1):\n",
    "        txt=''\n",
    "        start = '\\n'+str(n)\n",
    "        end = '\\n'+str(n+1)\n",
    "        p1, p2 = text_between_subtrings(s, start, end)\n",
    "        if p1!=-1 and p2!=-1:\n",
    "            txt=s[p1+len(start):p2]\n",
    "        if p1!=-1 and p2==-1:\n",
    "            txt=s[p1+len(start):]\n",
    "        anslist=[]\n",
    "        for m in range(0,4):\n",
    "            start=invkeydictionary[m]+')'\n",
    "            end=invkeydictionary[m+1]+')'\n",
    "            #looking for answer choices as the text between A) and B) for example\n",
    "            p1=txt.rfind(start)\n",
    "            p2=txt.rfind(end)\n",
    "            if p2!=-1:\n",
    "                ans=txt[p1:p2].strip()\n",
    "            else:\n",
    "                ans=txt[p1:].strip()\n",
    "            #if p2 was not found, do txt[p1:] rather than txt[p1:-1] which would truncate the last letter\n",
    "            ans=wordify_answers(ans)\n",
    "            if ans:\n",
    "                anslist.append(ans)\n",
    "            else:\n",
    "                anslist.append('')\n",
    "        if p2!=-1:\n",
    "            ans=txt[p2:].strip()\n",
    "            #ans=re.sub(' to power of ', '^', ans)\n",
    "            ans=wordify_answers(ans)\n",
    "            anslist.append(ans)\n",
    "        else:\n",
    "            anslist.append('')\n",
    "        all_choices.append(anslist)\n",
    "    return all_choices\n",
    "\n",
    "\n",
    "def getAllKeys(fixed_points_per_question=1):\n",
    "    '''read all files named key0-key4, make them into strings and and return a dictionary\n",
    "    containing a list of keys and also the number of questions'''\n",
    "    keylist=[]\n",
    "    pointlist=[]\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        if (os.path.isfile(filename)):\n",
    "            with open(filename, encoding=\"utf-8\") as file: \n",
    "                data = file.read()\n",
    "            u=re.search('\\n\\n\\n\\n+1\\)',data)\n",
    "            if u is not None:\n",
    "                answers=data[u.start():].strip()\n",
    "            else:\n",
    "                print('Make sure questions and answers in key'+str(n)+' are separated by \\n\\n\\n\\n')\n",
    "                return None\n",
    "\n",
    "            ukey=re.findall('[\\bA-E]',answers)\n",
    "            ukey=[str(ord(x)-65) for x in ukey]\n",
    "            key=''.join(ukey)\n",
    "            keylist.append(key)\n",
    "            \n",
    "            qnum=len(key)\n",
    "\n",
    "            #This assumes that the answerkey contains points EITHER in the user-supplied form 8) A /tab 3/n OR testgen-supplied form \"Points: 3\"\n",
    "            upoints=re.findall('\\t\\d+',answers)+re.findall(':\\s\\d+',answers)\n",
    "            if upoints:\n",
    "                _=''.join(upoints)\n",
    "                points=re.sub('[^ \\d+]',' ', _).split()\n",
    "                points=[int(x) for x in points ]\n",
    "                assert(len(points)==qnum)\n",
    "            else:\n",
    "                points=[fixed_points_per_question]*qnum\n",
    "            \n",
    "            points[-1]=0\n",
    "            pointlist.append(points)\n",
    "    \n",
    "    if not keylist:\n",
    "        print(\"Answerkeys not found. Make sure they are named key0.txt, key1.txt etc\")\n",
    "    return {\"keylist\":keylist,\"pointlist\":pointlist,\"numberOfQuestions\":qnum}\n",
    "\n",
    "def make_exempt_question_list(freeqs, All_IDs):\n",
    "    '''freeqs is an array of question numbers in version 0 that should be given full credit.\n",
    "    Using the cross-reference table for questions between versions (All_IDs) this function\n",
    "    creates a dictionary which lists which questions are to be given full credit for each version\n",
    "    Example {0:[1,3], 1:[2,7]...} where [1,3] were the free questions in v0 and [2,7] are the same questions in v1'''\n",
    "    \n",
    "    \n",
    "    exempt_list={}\n",
    "\n",
    "    if not freeqs:\n",
    "        return exempt_list\n",
    "    \n",
    "    qnos = set(range(1,len(All_IDs[0])+1))\n",
    "    if not set(freeqs).issubset(qnos):\n",
    "        print(\"Make sure to enter valid question numbers for version0(A)\")\n",
    "        return exempt_list\n",
    "    else:\n",
    "        freeqids = [x-1 for x in freeqs]\n",
    "        for m in range(0,len(QIDs)):\n",
    "            exempt_list[m] = [QIDs[m].index(x) for x in freeqids]\n",
    "        return exempt_list\n",
    "\n",
    "    \n",
    "\n",
    "def process_grades(data,outs, QIDs,analysis=False,exempt_questions={}):\n",
    "    '''grades all exams using correct keys, writes questions missed and scores'''\n",
    "\n",
    "    for NN in range(0, data.shape[0]):\n",
    "        #print(\"NN: \"+str(NN))\n",
    "        ans=data.iat[NN,2]\n",
    "        if len(ans)==numberOfQuestions-1:\n",
    "            v=guess_the_version(ans, exempt_questions)\n",
    "            ans=ans+str(v)\n",
    "            print(\"Assuming version \"+invkeydictionary[v]+ \" for: \"+data.iat[NN,1]+\" (Srl No: \"+data.iat[NN,0]+\").\")\n",
    "        check1=gradeWithKeylist(ans, outs, QIDs, analysis, NN, exempt_questions)\n",
    "        data.iat[NN,3]=check1['missed']\n",
    "        data.iat[NN,4]=check1['score']\n",
    "        data.iat[NN,5]=100.0*float(check1['score'])/float(new_totalpoints)\n",
    "\n",
    "    if analysis:\n",
    "        #remove the column with the version numbers\n",
    "        analysis_df.drop(analysis_df.columns[analysis_df.shape[1]-2], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def gradeWithKeylist(ans,outs, QIDs, analysis=False, N=0, exempt_questions={}):\n",
    "    '''multiple versions - find the correct key as indicated on the last question on the exam \n",
    "    exempt_questions is a dictionary that says which questions are free (i.e. full points) for each version'''\n",
    "    \n",
    "    keylist=outs[\"keylist\"]\n",
    "    pointlist=outs[\"pointlist\"]\n",
    "    numberOfQuestions=outs[\"numberOfQuestions\"] \n",
    "    \n",
    "    assert(keylist!=[])\n",
    "    assert (ans[-1:] in ['0','1','2','3','4'])\n",
    "    whichKey=int(ans[-1:])\n",
    "    \n",
    "    key=keylist[whichKey]\n",
    "    points=pointlist[whichKey]\n",
    "    assert len(key)==len(ans)\n",
    "    missed=\"\"\n",
    "    rejalt=[1]*numberOfQuestions\n",
    "    if not exempt_questions:\n",
    "        for n in range(0,len(key)-1):\n",
    "            if key[n]!=ans[n]:\n",
    "                missed+=str(n+1)+\", \"\n",
    "                rejalt[n]=0\n",
    "    else:\n",
    "        for n in range(0,len(key)-1):\n",
    "            if key[n]!=ans[n] and n not in exempt_questions[whichKey]:\n",
    "                missed+=str(n+1)+\", \"\n",
    "                rejalt[n]=0\n",
    "    if sum(rejalt)==numberOfQuestions:\n",
    "        missed=\"v\"+invkeydictionary[whichKey]+\": \"+\"ALL CORRECT\"\n",
    "    else:\n",
    "        missed=\"v\"+invkeydictionary[whichKey]+\": \"+missed[:len(missed)-2]\n",
    "    score=sum([i*j for i,j in zip(points,rejalt)])\n",
    "    \n",
    "\n",
    "    mydict1 = dict(zip(QIDs[whichKey],rejalt))\n",
    "    #sortedIDs=sorted(mydict1.keys())\n",
    "    sorted_rejalt=[mydict1[k] for k in QIDs[0]]####its sorted according to v0\n",
    "    \n",
    "\n",
    "    \n",
    "    if analysis:   \n",
    "        sorted_rejalt.append(score)\n",
    "        analysis_df.loc[N] = sorted_rejalt \n",
    "        \n",
    "        mydict2 = dict(zip(QIDs[whichKey],ans))  \n",
    "        #sortedIDs=sorted(mydict2.keys())\n",
    "        sorted_ans=[mydict2[k] for k in QIDs[0]]\n",
    "        allAnswers_df.loc[N] = sorted_ans\n",
    "    \n",
    "\n",
    "        \n",
    "    return {'missed':missed, 'score':score}\n",
    "\n",
    "\n",
    "def guess_the_version(no_version, exempt={}):\n",
    "    scores=[]\n",
    "    for n in range(numberOfVersions):\n",
    "        try_version=no_version+str(n)\n",
    "        check1=gradeWithKeylist(try_version, outs, QIDs, analysis=False, exempt_questions=exempt)\n",
    "        scores.append(check1['score'])\n",
    "    return scores.index(max(scores))\n",
    "\n",
    "\n",
    "\n",
    "def make_into_percent(adf):\n",
    "    '''Takes a dataframe and converts into np array to perform math operations.\n",
    "    In this case it divides each element by the sum of the row and times by 100 to make it a percent.'''\n",
    "    cols=list(adf)\n",
    "    #adf=adf.applymap(float)\n",
    "    np_df=adf.values\n",
    "    np_df=100*np_df/np.sum(np_df, axis=1, keepdims=True)\n",
    "    return pd.DataFrame(np_df,columns=cols)\n",
    "\n",
    "\n",
    "def mark_correct_answers(pardf,n,keylist):\n",
    "    \n",
    "    key_to_use=keylist[n]\n",
    "    for m in range(0,pardf.shape[0]):\n",
    "        cor=int(key_to_use[m])\n",
    "        pardf.iloc[m,cor]='[Correct] '+pardf.iloc[m,cor]\n",
    "    return pardf\n",
    "\n",
    "def analyse_items(a_df, koschen_paper, keylist):\n",
    "    '''create separate dfs with how many marked correct for each version separately'''\n",
    "    vers=['0','1','2','3','4']\n",
    "    lvers=['A','B','C','D','E']\n",
    "    outvars=[]\n",
    "    versionsfound=[]\n",
    "    students_in_this_version=[]\n",
    "\n",
    "    for n in range(0,len(koschen_paper)):\n",
    "        ch=str(n)\n",
    "        ##check for \"which version...\" to find the version and see if it matches ch\n",
    "        part_df = a_df.loc[a_df[list(a_df)[-1]]==ch]\n",
    "        if not part_df.empty:\n",
    "            print(\"Analysing data for version: \"+lvers[n])\n",
    "            part_df=part_df.T\n",
    "            part_df=part_df.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "            if ' ' in list(part_df):\n",
    "                part_df.rename(columns={' ': 'Blank'}, inplace=True)   \n",
    "            for pp in vers:\n",
    "                if pp in list(part_df):\n",
    "                    part_df.rename(columns={pp: lvers[int(pp)]}, inplace=True)\n",
    "            ####add back missing columns and rearrange in alphabetical order\n",
    "            cols_present=list(part_df)\n",
    "            allcols=['A','B','C','D','E','Blank']\n",
    "            cols_missing=[x for x in allcols if x not in cols_present]\n",
    "            if cols_missing:\n",
    "                for x in cols_missing:\n",
    "                    part_df[x]=0.0\n",
    "            part_df = part_df[allcols]\n",
    "            num_students_this_version=part_df.iloc[0,:].sum()\n",
    "            part_df=make_into_percent(part_df)\n",
    "            part_df=part_df.applymap('{:,.0f}%'.format)\n",
    "\n",
    "            kp=koschen_paper[n].copy()\n",
    "            kp.set_index(\"Question\", inplace=True)\n",
    "            kp.index.name=None ###This prevents an extra fictitious row from showing up in the header row\n",
    "            indexlist=kp.index.tolist()\n",
    "            part_df.index = indexlist\n",
    "            part_df=kp+\"<\"+part_df+\">\"\n",
    "            part_df=part_df.applymap(lambda m: re.sub('<0%>', '', m) )\n",
    "            \n",
    "            part_df=mark_correct_answers(part_df,n,keylist)\n",
    "            \n",
    "            \n",
    "            outvars.append(part_df)\n",
    "            versionsfound.append(lvers[n])\n",
    "            students_in_this_version.append(num_students_this_version)\n",
    "    \n",
    "    \n",
    "#     Return two dict so that the html function knows which version each table belongs to, and how many students took that version    \n",
    "    return dict(zip(versionsfound,outvars)), dict(zip(versionsfound,students_in_this_version))\n",
    "\n",
    "\n",
    "    \n",
    "def make_item_analysis(a_df):\n",
    "    \n",
    "    item_analysis_df=pd.DataFrame(index=[\"Difficulty\",\"Discrimination\"],columns = list(analysis_df))\n",
    "    n=a_df.shape[0]\n",
    "    v=analysis_df.values#converts into np array\n",
    "    \n",
    "    \n",
    "    #Calculating Difficulty\n",
    "    u1=np.sum(v,axis=0)/n\n",
    "    item_analysis_df.loc[\"Difficulty\"]=u1[0:u1.shape[0]]\n",
    "    \n",
    "    #Calculating Discrimination\n",
    "    students_per_group=n//3\n",
    "    if students_per_group<2:\n",
    "        print(\"Need more students for discrimination analysis (at least 7).\")\n",
    "    else:\n",
    "        v=v[0:v.shape[0]-1,:]#exclude the bottom row which had the sums (total number correct for each q)\n",
    "        v=v[v[:,v.shape[1]-1].argsort()[::-1]]#sort descending by scores\n",
    "        #print(v)\n",
    "        u1=np.sum(v[0:students_per_group,:],axis=0)#top scorers\n",
    "        u2=np.sum(v[n-students_per_group : n,:],axis=0)#bottom scorers\n",
    "        disc=(u1-u2)/students_per_group\n",
    "        disc.shape\n",
    "        item_analysis_df.loc[\"Discrimination\"]=disc[0:disc.shape[0]]\n",
    "    \n",
    "    item_analysis_df.drop(\"score\", axis=1, inplace=True)\n",
    "    return item_analysis_df.T\n",
    "\n",
    "#################Writing data#############################\n",
    "def write_to_xl(adf, rfilename):\n",
    "    adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    writer = pd.ExcelWriter(rfilename+'_processed.xlsx')\n",
    "    adf.to_excel(writer,'Sheet1',index=False)\n",
    "    writer.save()\n",
    "    \n",
    "def write_to_csv(adf, rfilename):\n",
    "    filename = rfilename+'_processed.csv'\n",
    "    #adf=adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    adf.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "def write_to_webpage(j_df, ia_df, pointdrop, ncbv={}, sbv={}, list_of_free_qs=[], histobin=11):\n",
    "    filename = rawdatafilename+'_summary.html'\n",
    "    f = open(filename,'w',encoding='utf8')\n",
    "    \n",
    "    max_score=max(j_df['Score'])\n",
    "    number_of_maxes=len([x for x in j_df['Score'] if x==max_score])\n",
    "    \n",
    "    uu=j_df['Percentage'].describe().to_frame()\n",
    "    uu.rename(columns={'Percentage': \"Value\"}, index={'count':'Number of Students','mean':'Mean(%)', 'std':'Standard Deviation', 'min':'Lowest(%)','25%':'25th percentile', '50%':'50th percentile', '75%':'75th percentile', 'max':\"Highest(%)\"},inplace=True)\n",
    "    uu.loc['Maximum Available(%) '] = [100*totalpoints/new_totalpoints]\n",
    "    uu.loc['Number of Top Scorers'] = number_of_maxes\n",
    "    uu.loc['Points dropped '] = pointdrop\n",
    "    if freebies:\n",
    "        uu.loc['Free Questions '] = len(list_of_free_qs)\n",
    "        \n",
    "\n",
    "    pre=\"<h2>Summary Data:</h2>\"\n",
    "    summary_table=uu.to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"sumdat\")\n",
    "    summary_table=pre+summary_table\n",
    "    post=\"<p>Score Distribution:</p>\"\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    df['Percentage'].plot(kind='hist', bins=10)\n",
    "    plt.xlim(xmax=100*totalpoints/new_totalpoints)\n",
    "    figure.savefig('histo.svg')\n",
    "    \n",
    "    pre=\"<h2>Score Distribution:</h2>\"\n",
    "    image_code='<img src=\"histo.svg\" alt=\"histogram\">'\n",
    "    image=pre+image_code\n",
    "\n",
    "    pre=\"<h2>Item Analysis:</h2>\"\n",
    "    item_analysis_table=ia_df.sort_values('Difficulty').to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"customers\")\n",
    "    item_analysis_table=pre+item_analysis_table\n",
    "\n",
    "    missed_tables=\"\"\n",
    "    if ncbv:\n",
    "        for ver in ['A', 'B','C','D','E']:\n",
    "            if ver in ncbv.keys():\n",
    "                if sbv:\n",
    "                    nn=str(int(sbv[ver]))\n",
    "                    pre=\"<h2>Distractor Analysis for Version \"+ver+\": (\"+nn+\" Students) </h2>\"\n",
    "                else:\n",
    "                    pre=\"<h2>Distractor Analysis for Version \"+ver+\":</h2>\"\n",
    "                post=ncbv[ver].to_html()\n",
    "                post=post.replace(\"dataframe\" ,\"howmany\")\n",
    "                post=post.replace('<td>[Correct]','<td class=\"correct\">')\n",
    "                post=post.replace('&lt;:','<span class=\"answered_by\">')\n",
    "                post=post.replace(':&gt;','</span>')\n",
    "                missed_tables+=pre+post\n",
    "\n",
    "    ##insert a script to make the webpage latex compatible. Edit the final  html file as \\( \\latex\\code \\)\n",
    "    html_start = \"\"\"<html>\n",
    "    <head>\n",
    "     <link rel=\"stylesheet\" type=\"text/css\" href=\"mystyle.css\">\n",
    "    <script type=\"text/javascript\" async\n",
    "    src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML\" async>\n",
    "    </script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Data Analysis</h1>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    html_end='''</body>\n",
    "    </html>'''\n",
    "    message=html_start+summary_table+image+item_analysis_table+missed_tables+html_end\n",
    "    f.write(message)\n",
    "    f.close()\n",
    "    \n",
    "\n",
    "    #Change path to reflect file location\n",
    "    webbrowser.open_new_tab(filename)\n",
    "\n",
    "###############################################\n",
    "\n",
    "############### Checking for incorrect Srl Nos################\n",
    "\n",
    "\n",
    "def check_serial_numbers(gradebook,datafile):\n",
    "    df=pd.read_csv(gradebook, usecols=[\"Last Name\", \"First Name\", \"Serial Number Text Grade <Text>\"])\n",
    "    df_all=pd.read_excel(datafile, header=None, parse_cols=1,names = [\"Serial Number Text Grade <Text>\", \"Name\"])\n",
    "\n",
    "    checker_left = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='left')\n",
    "    checker_right = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='right')\n",
    "    u1=checker_left[checker_left.isnull().any(axis=1)]\n",
    "    u2=checker_left[checker_left['Serial Number Text Grade <Text>'].duplicated(keep=False)]\n",
    "    u3=checker_right[checker_right.isnull().any(axis=1)]\n",
    "    error_rpt=pd.concat([u1, u2, u3], axis=0)\n",
    "    error_rpt[\"Last Name\"] = error_rpt[\"Last Name\"]+' '+error_rpt['First Name']\n",
    "    error_rpt.drop('First Name', axis=1,inplace=True)\n",
    "    error_rpt.rename(columns={'Serial Number Text Grade <Text>': 'Srl No', 'Last Name':'Registered Name', 'Name':'Entered Name'}, inplace=True) \n",
    "    return error_rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually convert the dat file into an excel file using excel. Only extract the serial number,\n",
    "name and Answers, and make sure Answers is text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1009</td>\n",
       "      <td>HAN KARIN</td>\n",
       "      <td>0110241 21 4040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Srl No       Name          Answers\n",
       "2  1009   HAN KARIN  0110241 21 4040"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Registered Name</th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Entered Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolling Atiyaa</td>\n",
       "      <td>1003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Muezzinoglu Mine</td>\n",
       "      <td>1018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Carlson Brynn</td>\n",
       "      <td>1002</td>\n",
       "      <td>CARLSON BRYNN L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carlson Brynn</td>\n",
       "      <td>1002</td>\n",
       "      <td>MUEZZINOGLU MINE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Registered Name  Srl No      Entered Name\n",
       "1   Bolling Atiyaa    1003    NaN             \n",
       "12  Muezzinoglu Mine  1018    NaN             \n",
       "2   Carlson Brynn     1002    CARLSON BRYNN L \n",
       "3   Carlson Brynn     1002    MUEZZINOGLU MINE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#COLLECTING DATA\n",
    "rawdatafilename='All'\n",
    "datafilename=rawdatafilename+'.xlsx'\n",
    "gradebook='BSGrades.csv'\n",
    "if not (os.path.isfile(datafilename)) or not (os.path.isfile(gradebook)):\n",
    "    print('Make sure All.xlsx and BSGrades.csv are ready first')\n",
    "    exit(\"Necessary files not found\")\n",
    "xls_file = pd.ExcelFile(datafilename)\n",
    "df = xls_file.parse('Sheet1', header=None, parse_cols=2,names = [\"Srl No\", \"Name\", \"Answers\"], dtype='str')\n",
    "#parse_cols makes sure that only cols 0,1 and 2 are extracted\n",
    "#checking for blanks, print only if blanks found\n",
    "if not df[df['Answers'].str.contains(\" \")].empty:\n",
    "    blankers=df[df['Answers'].str.contains(\" \")]\n",
    "    display(blankers)\n",
    "ER=check_serial_numbers(gradebook,datafilename)\n",
    "if not ER.empty:\n",
    "    display(ER)\n",
    "\n",
    "df[\"Missed\"] = \"\"\n",
    "df[\"Score\"]=0\n",
    "df[\"Percentage\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating QIDs. This might take a while..\n",
      "Done. That took: 5.246004581451416 sec.\n"
     ]
    }
   ],
   "source": [
    "#initializing everything\n",
    "keydictionary={\"A\":\"0\",\"B\":\"1\",\"C\":\"2\",\"D\":\"3\",\"E\":\"4\"}\n",
    "invkeydictionary={0:\"A\",1:\"B\",2:\"C\",3:\"D\",4:\"E\"}\n",
    "points_per_question=3\n",
    "point_drop=2\n",
    "outs = getAllKeys(points_per_question)\n",
    "keylist=outs[\"keylist\"]\n",
    "pointlist=outs[\"pointlist\"]\n",
    "numberOfQuestions=outs[\"numberOfQuestions\"]\n",
    "\n",
    "freebies=[4,5]#enter ACTUAL QUESTION NUMBER of questions from version 0(A) to which to award full credit regardless of response\n",
    "totalpoints=sum(pointlist[2])\n",
    "new_totalpoints=totalpoints-point_drop\n",
    "numberOfVersions=len(keylist)\n",
    "numberOfStudents=df.shape[0]\n",
    "quids_dfs=createQuestionIDs_and_dfs(max_words=50)\n",
    "QIDs=quids_dfs[\"IDs\"]\n",
    "koschen_paper=quids_dfs['dfs']\n",
    "if not keylist or not pointlist or numberOfQuestions==0 or not QIDs or not koschen_paper:\n",
    "    exit(\"Something went wrong in importing the questions\")\n",
    "headings=QIDs[0].copy()#####headings=QIDs[0] assigns by reference, so changing headings will change QIDs[0]\n",
    "allAnswers_df = pd.DataFrame(index=range(numberOfStudents), columns = headings)# stores all student answers for each question, questions are the columns\n",
    "headings.append(\"score\")\n",
    "analysis_df  = pd.DataFrame(index=range(numberOfStudents),columns = headings)# stores whether answer was correct, questions are the columns, last column is the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1002</td>\n",
       "      <td>MUEZZINOGLU MINE</td>\n",
       "      <td>02022401103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1001</td>\n",
       "      <td>ACQUAYE AMBER</td>\n",
       "      <td>23100110212344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Srl No              Name         Answers\n",
       "5   1002   MUEZZINOGLU MINE  02022401103400\n",
       "18  1001   ACQUAYE AMBER     23100110212344"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculations...\n",
      "Assuming version B for: MUEZZINOGLU MINE (Srl No: 1002).\n",
      "Assuming version A for: ACQUAYE AMBER (Srl No: 1001).\n",
      "Analysing data for version: A\n",
      "Analysing data for version: B\n",
      "Analysing data for version: C\n",
      "Analysing data for version: D\n",
      "Done. That took: 0.185502290725708 sec for 19 students\n",
      "Warning: Duplicates in serial numbers found!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Answers</th>\n",
       "      <th>Missed</th>\n",
       "      <th>Score</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1002</td>\n",
       "      <td>CARLSON BRYNN L</td>\n",
       "      <td>334310202024403</td>\n",
       "      <td>vD: ALL CORRECT</td>\n",
       "      <td>42</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1002</td>\n",
       "      <td>MUEZZINOGLU MINE</td>\n",
       "      <td>02022401103400</td>\n",
       "      <td>vB: 2, 10</td>\n",
       "      <td>36</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Srl No              Name          Answers           Missed  Score  \\\n",
       "4  1002   CARLSON BRYNN L   334310202024403  vD: ALL CORRECT  42      \n",
       "5  1002   MUEZZINOGLU MINE  02022401103400   vB: 2, 10        36      \n",
       "\n",
       "   Percentage  \n",
       "4  105.0       \n",
       "5  90.0        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Everything initialized. Running the code now\n",
    "didnt_enter_versions=df[df['Answers'].map(len)!=numberOfQuestions]\n",
    "if not didnt_enter_versions.empty:\n",
    "    didnt_enter_versions=didnt_enter_versions[['Srl No', 'Name','Answers']]\n",
    "    display(didnt_enter_versions)\n",
    "print(\"Starting calculations...\")\n",
    "starttime = time()\n",
    "exempts=make_exempt_question_list(freebies, QIDs)\n",
    "df=process_grades(df,outs, QIDs, analysis=True,exempt_questions=exempts )\n",
    "number_correct_by_version, students_by_version = analyse_items(allAnswers_df, koschen_paper,keylist)\n",
    "item_analysis_df = make_item_analysis(analysis_df) \n",
    "endtime = time()\n",
    "print(\"Done. That took: \"+str(endtime-starttime)+ \" sec for \"+str(numberOfStudents)+\" students\")\n",
    "#check for duplicated serial numbers\n",
    "dupes=df[df['Srl No'].duplicated(keep=False)]\n",
    "if not dupes.empty:\n",
    "    print('Warning: Duplicates in serial numbers found!')\n",
    "    display(dupes)    \n",
    "max_score=analysis_df.at[analysis_df['score'].idxmax(),'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24d5d543438>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# write_to_csv(df.sort_values('Score', ascending=False),rawdatafilename)\n",
    "if (os.path.isfile('mystyle.css')):\n",
    "    write_to_webpage(df, item_analysis_df, point_drop, number_correct_by_version, students_by_version, list_of_free_qs=freebies)\n",
    "else:\n",
    "    print(\"Copy the css file over first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grades and displays the N'th entry in the list using a keylist\n",
    "#N=random.randint(1,df.shape[0]-1)\n",
    "#for N in range(df.shape[0]-1):\n",
    "N=100\n",
    "check1=gradeWithKeylist(df.iat[N,2], outs, QIDs, analysis=False, exempt_questions=exempts)\n",
    "print(\"Row \"+ str(N)+\": \"+df.iat[N,1]+\", \"+str(df.iat[N,0])+\". Missed \"+str(check1['missed'])+ \". Scored \"+str(check1['score'])+\"/\"+str(totalpoints))\n",
    "print('That is: '+str(100*check1['score']/new_totalpoints)+'% with '+str(point_drop)+' points dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DEV AREA###########################################\n",
    "\n",
    "\n",
    "# filename=\"key\"+str(0)+\".txt\"\n",
    "# if (os.path.isfile(filename)):\n",
    "#     with open(filename, encoding=\"utf-8\") as file:  \n",
    "#         data = file.read()\n",
    "#     p=data.rfind('\\n\\n\\n\\n1)')\n",
    "# s=data[:p]\n",
    "\n",
    "# n=25\n",
    "# start = '\\n'+str(n)\n",
    "# end = '\\n'+str(n+1)\n",
    "# p1, p2 = text_between_subtrings(s, start, end)\n",
    "# if p1!=-1 and p2!=-1:\n",
    "#     txt=s[p1+len(start):p2]\n",
    "# if p1!=-1 and p2==-1:\n",
    "#     txt=s[p1+len(start):]\n",
    "\n",
    "# print(txt)\n",
    "# anslist=[]\n",
    "# for m in range(0,4):\n",
    "#     start=invkeydictionary[m]+')'\n",
    "#     end=invkeydictionary[m+1]+')'\n",
    "#     #looking for answer choices as the text between A) and B) for example\n",
    "#     p1=txt.rfind(start)\n",
    "#     p2=txt.rfind(end)\n",
    "#     if p2!=-1:\n",
    "#         ans=txt[p1:p2].strip()\n",
    "#     else:\n",
    "#         ans=txt[p1:].strip()\n",
    "#     #if p2 was not found, do txt[p1:] rather than txt[p1:-1] which would truncate the last letter\n",
    "#     ans=wordify_answers(ans)\n",
    "#     if ans:\n",
    "#         print('p1= '+str(p1)+', p2= '+str(p2)+\" ans=\"+ans+' z1')\n",
    "#         anslist.append(ans)\n",
    "#     else:\n",
    "#         print('p1= '+str(p1)+', p2= '+str(p2)+\" ans= None\"+' z1')\n",
    "#         anslist.append('')\n",
    "# if p2!=-1:\n",
    "#     ans=txt[p2:].strip()\n",
    "#     #ans=re.sub(' to power of ', '^', ans)\n",
    "#     ans=wordify_answers(ans)\n",
    "#     print('p1= '+str(p1)+', p2= '+str(p2)+\" ans=\"+ans+' z2')\n",
    "#     anslist.append(ans)\n",
    "# else:\n",
    "#     print('p1= '+str(p1)+', p2= '+str(p2)+\" ans= None\"+' z3')\n",
    "#     anslist.append('')\n",
    "\n",
    "\n",
    "\n",
    "vers=['0','1','2','3','4']\n",
    "lvers=['A','B','C','D','E']\n",
    "n=0\n",
    "a_df=allAnswers_df\n",
    "ch=str(n)\n",
    "# ##check for \"which version...\" to find the version and see if it matches ch\n",
    "# ver_qnum=koschen_paper[n].index[koschen_paper[n]['Question'].str.contains(\"version is this\")].tolist()[0]-1\n",
    "# #return the indexes of the cells which contain the given text, as a list\n",
    "# ver_q=koschen_paper[n].iloc[ver_qnum,0]\n",
    "# ver_q=process.extractOne(ver_q, list(a_df))[0]\n",
    "part_df = a_df.loc[a_df[list(a_df)[-1]]==ch]\n",
    "if not part_df.empty:\n",
    "    print(\"Analysing data for version: \"+lvers[n])\n",
    "    part_df=part_df.T\n",
    "    part_df=part_df.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "    if ' ' in list(part_df):\n",
    "        part_df.rename(columns={' ': 'Blank'}, inplace=True)   \n",
    "    for pp in vers:\n",
    "        if pp in list(part_df):\n",
    "            part_df.rename(columns={pp: lvers[int(pp)]}, inplace=True)\n",
    "    ####add back missing columns and rearrange in alphabetical order\n",
    "    cols_present=list(part_df)\n",
    "    allcols=['A','B','C','D','E','Blank']\n",
    "    cols_missing=[x for x in allcols if x not in cols_present]\n",
    "    if cols_missing:\n",
    "        for x in cols_missing:\n",
    "            part_df[x]=0.0\n",
    "    part_df = part_df[allcols]\n",
    "#     part_df['Question']=list(koschen_paper[n]['Question'])\n",
    "#     part_df.set_index('Question', inplace=True)\n",
    "#     part_df = part_df.reindex(koschen_paper[n][\"Question\"])\n",
    "    num_students_this_version=part_df.iloc[0,:].sum()\n",
    "    part_df=make_into_percent(part_df)\n",
    "    part_df=part_df.applymap('{:,.0f}%'.format)\n",
    "\n",
    "    kp=koschen_paper[n].copy()\n",
    "    kp.set_index(\"Question\", inplace=True)\n",
    "    kp.index.name=None ###This prevents an extra fictitious row from showing up in the header row\n",
    "    indexlist=kp.index.tolist()\n",
    "    part_df.index = indexlist\n",
    "    part_df=kp+\"<\"+part_df+\">\"\n",
    "    part_df=part_df.applymap(lambda m: re.sub('<0%>', '', m) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_new=[]\n",
    "qid_list=[x for x in range(numberOfQuestions)]\n",
    "for whichlist in QIDs[1:]:\n",
    "    qid_list=[]\n",
    "    for m in range(0,len(QIDs[0])):\n",
    "        matching_question=process.extractOne(QIDs[0][m], whichlist)[0]\n",
    "        qid_list.append(whichlist.index(matching_question))\n",
    "    qids_new.append(qid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freeqs=[0,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnos = set(range(1,len(QIDs[0])+1))\n",
    "qnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qnos=set(range(1,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(freeqs).issubset(qnos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempt_list={}\n",
    "for m in range(0,len(QIDs)):\n",
    "    exempt_list[m]=[QIDs[m].index(x) for x in freeqids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_exempt_question_list(freeqs, All_IDs):\n",
    "    exempt_list={}\n",
    "\n",
    "    if not freeqs:\n",
    "        return exempt_list\n",
    "    \n",
    "    qnos = set(range(1,len(All_IDs[0])+1))\n",
    "    if not set(freeqs).issubset(qnos):\n",
    "        print(\"Make sure to enter valid question numbers for version0(A)\")\n",
    "        return exempt_list\n",
    "    else:\n",
    "        freeqids = [x-1 for x in freeqs]\n",
    "        for m in range(0,len(QIDs)):\n",
    "            exempt_list[m] = [QIDs[m].index(x) for x in freeqids]\n",
    "        return exempt_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichkey=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exempt_list[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 not in exempt_list[whichkey]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=koschen_paper[0].loc[1,\"Question\"]\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu=list(koschen_paper[1].loc[:,\"Question\"])\n",
    "uu.index(process.extractOne(m,uu)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uu[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0=list(koschen_paper[0].loc[:,\"Question\"])\n",
    "v1=list(koschen_paper[1].loc[:,\"Question\"])\n",
    "v2=list(koschen_paper[2].loc[:,\"Question\"])\n",
    "v3=list(koschen_paper[3].loc[:,\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v3[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v0[0])\n",
    "print(v1[QIDs[1].index(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QIDs[1].index(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_IDs=[v0, v1, v2, v3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_IDs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_new=[]\n",
    "qid_list=[x for x in range(0, 15)]\n",
    "qids_new.append(qid_list)\n",
    "\n",
    "for whichlist in All_IDs[1:]:\n",
    "    qid_list=[1]*len(All_IDs[0])\n",
    "    for m in range(0,len(All_IDs[0])):\n",
    "        matching_question=process.extractOne(All_IDs[0][m], whichlist)[0]\n",
    "        qid_list[whichlist.index(matching_question)]=m\n",
    "    qids_new.append(qid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_IDs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "len(All_IDs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichlist=All_IDs[1]\n",
    "matching_question=process.extractOne(All_IDs[0][m], whichlist)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whichlist.index(matching_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list[whichlist.index(matching_question)]=m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [3, 4], 1: [8, 13], 2: [3, 8], 3: [0, 2], 4: [3, 4]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
