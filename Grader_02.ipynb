{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "import os.path\n",
    "import random\n",
    "import time\n",
    "import string\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "\n",
    "#All Function definitions\n",
    "\n",
    "def generateIDs(keyname, numberOfQuestions):\n",
    "    '''generates a unique ID for each question using the beginning characters of each question'''\n",
    "    goodIDs=[]\n",
    "    with open(keyname) as file:  \n",
    "        data = file.read()\n",
    "    for n in range(1,numberOfQuestions+1):\n",
    "            #print(\"processing: \"+str(n))\n",
    "            locs=[]\n",
    "            occursat=find_all(data, str(n)+')')\n",
    "            for ii in occursat:\n",
    "                if data[ii-1]=='\\r' or data[ii-1]=='\\n'  and data[ii+3]!=\"_\":# check for both /r and /n\n",
    "                    locs.append(ii)\n",
    "                    #print(locs)\n",
    "            z=lambda x: str('0')+str(x) if x<10 else str(x)\n",
    "            theID=z(n)+\"_\"+(data[locs[0]+3:locs[0]+50]).strip()\n",
    "            #print(theID)\n",
    "            goodIDs.append(theID)\n",
    "    return goodIDs\n",
    "\n",
    "def unduplicate(dup_list):\n",
    "    '''finds duplicate entries in a list and adds characters to duplicated entries to make them unique.\n",
    "    works in a roundabout way by converting to a dataframe first , and then converting back to a list in the end'''\n",
    "    df = pd.DataFrame(dup_list)\n",
    "    dups=df[df.duplicated(keep='first')]\n",
    "    if not dups.empty:\n",
    "        dup_indices=list(dups.index)\n",
    "        for i in dup_indices:\n",
    "            df.iat[i,0]=df.iat[i,0]+str(i)\n",
    "            print(\"Unduplicating: \"+ df.iat[i,0])\n",
    "    undup_list=list(df.values.flatten())\n",
    "    return undup_list\n",
    "\n",
    "def createQuestionIDs(numberOfQuestions):\n",
    "    '''this will generateIDs unique question IDs for each question from the raw keys'''\n",
    "    allIDs=[]\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        if (os.path.isfile(filename)): \n",
    "            qid=generateIDs(filename,numberOfQuestions)\n",
    "            allIDs.append(qid)\n",
    "    allIDs[0]=unduplicate(allIDs[0])\n",
    " #The ID's generated are slightly different in each version, so set them all equal to the ID's in version0           \n",
    "    for n in range(1,numberOfVersions):\n",
    "        for m in range(0,numberOfQuestions):\n",
    "            sub=allIDs[0][m][5:len(allIDs[0][m])-len(allIDs[0][m])//5]\n",
    "            ########### THe 5 and 10 are chosen without any logic - some logic would be nice\n",
    "            #print(sub)\n",
    "            matching = [s for s in allIDs[n] if sub in s]\n",
    "            found_in=allIDs[n].index(matching[0])\n",
    "            if allIDs[n][found_in]!=allIDs[0][m]:\n",
    "                    #print(allIDs[n][found_in]+\"   changed to    \"+allIDs[0][m])\n",
    "                    allIDs[n][found_in]=allIDs[0][m]\n",
    "                    \n",
    "    \n",
    "    return allIDs\n",
    "\n",
    "def find_all(a_str, sub):\n",
    "    '''finds all instances of a substring in a string'''\n",
    "    start = 0\n",
    "    result=[]\n",
    "    while True:\n",
    "        start = a_str.find(sub, start)\n",
    "        if start == -1: return result\n",
    "        result.append(start)\n",
    "        start += len(sub) # use start += 1 to find overlapping matches\n",
    "    return result\n",
    "\n",
    "def cleanKey(key_name):\n",
    "    '''retrieves just the \"key\" part of exported test. \n",
    "    Just reads and returns the key if the questions have already been deleted \n",
    "    writes to a new file with name cleankey'''\n",
    "    if(os.path.isfile(key_name)):\n",
    "        #print(\"Scrubbing: \"+key_name+\" which is :\" + str(type(key_name)))\n",
    "        with open(key_name) as file:  \n",
    "            data = file.read()\n",
    "            p=data.rfind('_____')\n",
    "            if p!=-1: \n",
    "                data=data[p+5:]#delete all the junk before \"_______\"\n",
    "                data=data[data.find('1)'):]#keep all the stuff after the 1)\n",
    "        endline=['\\rPoints: ', '\\nPoints: ']\n",
    "        for pp in endline:\n",
    "            if data.find(pp)!=-1:# remove the question ID to a separate column if necessary\n",
    "                data = data.replace(pp,'\\t')\n",
    "        with open('cleaned_'+key_name, 'w') as file:\n",
    "            file.write(data)\n",
    "    else:\n",
    "        print(str(key_name)+\" not found\")\n",
    "        \n",
    "def getAllKeys(fixed_points_per_question=1):\n",
    "    '''read all files named key0-key4, make them into strings and and return a dictionary\n",
    "    containing a list of keys and also the number of questions'''\n",
    "    keylist=[]\n",
    "    pointlist=[]\n",
    "    numberOfQuestions=0\n",
    "\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        #print(\"processing: \"+filename)\n",
    "        if os.path.isfile(filename): \n",
    "            cleanKey(filename)#always create a fresh cleankey, to avoid using an outdated one\n",
    "            pp=pd.read_table(\"cleaned_\"+filename, header=None)\n",
    "            keys=pp.iloc[:,0].to_frame()#converts a series to a dataframe\n",
    "            keylist.append(makekey(keys))\n",
    "            if pp.shape[1]>1:\n",
    "                points=pp.iloc[:,1].to_frame()\n",
    "                pointlist.append(makepoints(points))\n",
    "            else:\n",
    "                points=[fixed_points_per_question]*pp.shape[0]\n",
    "                points[-1]=0\n",
    "                pointlist.append(points)\n",
    "\n",
    "    numberOfQuestions=keys.shape[0]\n",
    "    if not keylist:\n",
    "        print(\"Answerkeys not found. Make sure they are named key0.txt, key1.txt etc\")\n",
    "    getAllKeysOutput={\"keylist\":keylist,\"pointlist\":pointlist,\"numberOfQuestions\":numberOfQuestions}\n",
    "    return getAllKeysOutput\n",
    "\n",
    "\n",
    "def makekey(key_from_test):\n",
    "    '''key_from_test is a dataframe formed from reading the answerkey file. This function converts it into a text string with numbers 0-5 representing A-E'''\n",
    "    #keydictionary={\"A\":\"0\",\"B\":\"1\",\"C\":\"2\",\"D\":\"3\",\"E\":\"4\"}\n",
    "    thekey=\"\"\n",
    "    for i in range(0,key_from_test.shape[0]):\n",
    "        #print(\"At i=\"+str(i))\n",
    "        #get the last letter, i.e. the \"C\" from \"1)C\" \n",
    "        thekey+=keydictionary[key_from_test.iat[i,0][-1:]]\n",
    "    return thekey\n",
    "\n",
    "def makepoints(points):\n",
    "    '''pointlist_from_test is a dataframe formed from reading the answerkey file. This function converts it into a text string with numbers 0-5 representing A-E'''\n",
    "    #keydictionary={\"A\":\"0\",\"B\":\"1\",\"C\":\"2\",\"D\":\"3\",\"E\":\"4\"}\n",
    "    pointlist=[]\n",
    "    for i in range(0,points.shape[0]):\n",
    "        #print(\"At i=\"+str(i))\n",
    "        #get the last letter, i.e. the \"C\" from \"1)C\" \n",
    "        pointlist.append(int(points.iat[i,0]))\n",
    "    pointlist[-1]=0\n",
    "    return pointlist\n",
    "\n",
    "\n",
    "def process_grades(data,outs, QIDs,analysis=False):\n",
    "    '''grades all exams using correct keys, writes questions missed and scores'''\n",
    "\n",
    "    for NN in range(0, data.shape[0]):\n",
    "        #print(\"NN: \"+str(NN))\n",
    "        ans=data.iat[NN,2]\n",
    "        if len(ans)==numberOfQuestions-1:\n",
    "            v=guess_the_version(ans)\n",
    "            ans=ans+str(v)\n",
    "            print(\"Assuming version \"+invkeydictionary[v]+ \" for: \"+data.iat[NN,1]+\" (Srl No: \"+data.iat[NN,0]+\").\")\n",
    "        check1=gradeWithKeylist(ans, outs, QIDs, analysis, NN)\n",
    "        data.iat[NN,3]=check1['missed']\n",
    "        data.iat[NN,4]=check1['score']\n",
    "        data.iat[NN,5]=100.0*float(check1['score'])/float(new_totalpoints)\n",
    "\n",
    "    if analysis:\n",
    "        #remove the column with the version numbers\n",
    "        analysis_df.drop(analysis_df.columns[analysis_df.shape[1]-2], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def gradeWithKeylist(ans,outs, QIDs, analysis=False, N=0):\n",
    "    '''multiple versions - find the correct key as indicated on the last question on the exam '''\n",
    "    keylist=outs[\"keylist\"]\n",
    "    pointlist=outs[\"pointlist\"]\n",
    "    numberOfQuestions=outs[\"numberOfQuestions\"] \n",
    "    \n",
    "    assert(keylist!=[])\n",
    "    assert (ans[-1:] in ['0','1','2','3','4'])\n",
    "    whichKey=int(ans[-1:])\n",
    "    \n",
    "    key=keylist[whichKey]\n",
    "    points=pointlist[whichKey]\n",
    "    assert len(key)==len(ans)\n",
    "    missed=\"\"\n",
    "    rejalt=[1]*numberOfQuestions\n",
    "    for n in range(0,len(key)-1):\n",
    "        if key[n]!=ans[n]:\n",
    "            #print(\"storing\")\n",
    "            missed+=str(n+1)+\", \"\n",
    "            rejalt[n]=0\n",
    "    if sum(rejalt)==numberOfQuestions:\n",
    "        missed=\"ALL CORRECT\"\n",
    "    else:\n",
    "        missed=\"v\"+invkeydictionary[whichKey]+\": \"+missed[:len(missed)-2]\n",
    "    score=sum([i*j for i,j in zip(points,rejalt)])\n",
    "    \n",
    "\n",
    "    mydict1 = dict(zip(QIDs[whichKey],rejalt))\n",
    "    sortedIDs=sorted(mydict1.keys())\n",
    "    sorted_rejalt=[mydict1[k] for k in sortedIDs]####its sorted according to v0\n",
    "    \n",
    "\n",
    "    \n",
    "    if analysis:   \n",
    "        sorted_rejalt.append(score)\n",
    "        analysis_df.loc[N] = sorted_rejalt \n",
    "        \n",
    "        mydict2 = dict(zip(QIDs[whichKey],ans))  \n",
    "        sortedIDs=sorted(mydict2.keys())\n",
    "        sorted_ans=[mydict2[k] for k in sortedIDs]\n",
    "        allAnswers_df.loc[N] = sorted_ans\n",
    "    \n",
    "\n",
    "        \n",
    "    return {'missed':missed, 'score':score}\n",
    "\n",
    "\n",
    "def guess_the_version(no_version):\n",
    "    scores=[]\n",
    "    for n in range(numberOfVersions):\n",
    "        try_version=no_version+str(n)\n",
    "        check1=gradeWithKeylist(try_version, outs, QIDs, analysis=False)\n",
    "        scores.append(check1['score'])\n",
    "    return scores.index(max(scores))\n",
    "\n",
    "def count_how_many(col, value):\n",
    "    '''takes a pandas series \"col\" and counts the number of instances of a \"value\" \n",
    "    Not used since pandas has a built-in function'''\n",
    "    \n",
    "    count=0\n",
    "    for n in range(0,col.shape[0]):\n",
    "        if col[n]==value:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "\n",
    "def addStarsToCorrectChoices(rdf, keylist, m, QIDS):\n",
    "    '''adds stars to correct choices in rdf'''\n",
    "    for n in range(0,rdf.shape[0]):\n",
    "        if m!=0:\n",
    "            mydict0 = dict(zip(QIDs[m],keylist[m])) \n",
    "            sortedkey=[mydict0[x] for x in QIDs[0]]\n",
    "        else:\n",
    "            sortedkey=keylist[m]\n",
    "        the_correct_answer=sortedkey[n]\n",
    "        rdf.iloc[n][the_correct_answer]=rdf.iloc[n][the_correct_answer]+\"*\"\n",
    "    return rdf\n",
    "\n",
    "def analyse_items(a_df, QIDs):\n",
    "    '''create separate dfs with how many marked correct for each version separately'''\n",
    "    vers=['0','1','2','3','4']\n",
    "    lvers=['A','B','C','D','E']\n",
    "    outvars=[]\n",
    "    print(\"Versions found:\")\n",
    "    for n in range(0,len(vers)):\n",
    "        ch=str(n)\n",
    "        ##check the last column \"which version...\" to find the version and see if it matches ch\n",
    "        #part_df = a_df.loc[allAnswers_df[list(allAnswers_df)[-1]]==ch]\n",
    "        part_df = a_df.loc[a_df[list(a_df)[-1]]==ch]\n",
    "        if not part_df.empty:\n",
    "            print(lvers[n])\n",
    "            part_df=part_df.T\n",
    "            part_df=part_df.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "            part_df=part_df.applymap(int)\n",
    "            part_df=part_df.applymap(str)\n",
    "            part_df=addStarsToCorrectChoices(part_df,keylist,n, QIDs)\n",
    "            if ' ' in list(part_df):\n",
    "                part_df.rename(columns={' ': 'blank'}, inplace=True)   \n",
    "            for pp in vers:\n",
    "                if pp in list(part_df):\n",
    "                    part_df.rename(columns={pp: lvers[int(pp)]}, inplace=True)\n",
    "            outvars.append(part_df)\n",
    "        \n",
    "    return outvars\n",
    "\n",
    "def make_item_analysis(a_df):\n",
    "    \n",
    "    item_analysis_df=pd.DataFrame(index=[\"Difficulty\",\"Discrimination\"],columns = list(analysis_df))\n",
    "    n=a_df.shape[0]\n",
    "    v=analysis_df.values#converts into np array\n",
    "    \n",
    "    \n",
    "    #Calculating Difficulty\n",
    "    u1=np.sum(v,axis=0)/n\n",
    "    item_analysis_df.loc[\"Difficulty\"]=u1[0:u1.shape[0]]\n",
    "    \n",
    "    #Calculating Discrimination\n",
    "    students_per_group=n//3\n",
    "    if students_per_group<2:\n",
    "        print(\"Need more students for discrimination analysis (at least 7).\")\n",
    "    else:\n",
    "        v=v[0:v.shape[0]-1,:]#exclude the bottom row which had the sums (total number correct for each q)\n",
    "        v=v[v[:,v.shape[1]-1].argsort()[::-1]]#sort descending by scores\n",
    "        #print(v)\n",
    "        u1=np.sum(v[0:students_per_group,:],axis=0)#top scorers\n",
    "        u2=np.sum(v[n-students_per_group : n,:],axis=0)#bottom scorers\n",
    "        disc=(u1-u2)/students_per_group\n",
    "        disc.shape\n",
    "        item_analysis_df.loc[\"Discrimination\"]=disc[0:disc.shape[0]]\n",
    "    \n",
    "    item_analysis_df.drop(\"score\", axis=1, inplace=True)\n",
    "    return item_analysis_df.T\n",
    "\n",
    "#################Writing data#############################\n",
    "def write_to_xl(adf, rfilename):\n",
    "    adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    writer = pd.ExcelWriter(rfilename+'_processed.xlsx')\n",
    "    adf.to_excel(writer,'Sheet1',index=False)\n",
    "    writer.save()\n",
    "    \n",
    "def write_to_csv(adf, rfilename):\n",
    "    filename = rfilename+'_processed.csv'\n",
    "    #adf=adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    adf.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "def write_to_webpage(j_df, a_df,ia_df,ncbv, histobin=11):\n",
    "    filename = rawdatafilename+'_summary.html'\n",
    "    f = open(filename,'w')\n",
    "    \n",
    "    uu=df['Percentage'].describe().to_frame()\n",
    "    uu.rename(columns={'Percentage': \"Value\"}, index={'count':'Number of Students','mean':'Mean(%)', 'std':'Standard Deviation', 'min':'Lowest(%)','25%':'25th percentile', '50%':'50th percentile', '75%':'75th percentile', 'max':\"Highest(%)\"},inplace=True)\n",
    "    uu.loc['Maximum Available(%) '] = [100*totalpoints/new_totalpoints]\n",
    "    uu.loc['Points dropped '] = [point_drop]\n",
    "\n",
    "    pre=\"<h2>Summary Data:</h2>\"\n",
    "    summary_table=uu.to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"sumdat\")\n",
    "    summary_table=pre+summary_table\n",
    "    post=\"<p>Score Distribution:</p>\"\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    df['Percentage'].plot(kind='hist', bins=10)\n",
    "    plt.xlim(xmax=100*totalpoints/new_totalpoints)\n",
    "    figure.savefig('histo.svg')\n",
    "    \n",
    "    pre=\"<h2>Score Distribution:</h2>\"\n",
    "    image_code='<img src=\"histo.svg\" alt=\"histogram\">'\n",
    "    image=pre+image_code\n",
    "\n",
    "    pre=\"<h2>Item Analysis:</h2>\"\n",
    "    item_analysis_table=ia_df.sort_values('Difficulty').to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"customers\")\n",
    "    item_analysis_table=pre+item_analysis_table\n",
    "\n",
    "    missed_tables=\"\"\n",
    "    for nc in range(len(ncbv)):\n",
    "        pre=\"<h2>Distractor Analysis for Version \"+invkeydictionary[nc]+\":</h2>\"\n",
    "        post=ncbv[nc].to_html().replace(\"dataframe\" ,\"howmany\")\n",
    "        missed_tables+=pre+post\n",
    "\n",
    "\n",
    "    html_start = \"\"\"<html>\n",
    "    <head>\n",
    "     <link rel=\"stylesheet\" type=\"text/css\" href=\"mystyle.css\">\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Data Analysis</h1>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    html_end='''</body>\n",
    "    </html>'''\n",
    "    message=html_start+summary_table+image+item_analysis_table+missed_tables+html_end\n",
    "    f.write(message)\n",
    "    f.close()\n",
    "\n",
    "    #Change path to reflect file location\n",
    "    webbrowser.open_new_tab(filename)\n",
    "\n",
    "###############################################\n",
    "## Functions that create fake data#############\n",
    "\n",
    "def create_afake(number_of_versions, blanks=False):\n",
    "    '''creates a fake answerkey'''\n",
    "    ansstring=[]\n",
    "    if not blanks:\n",
    "        choices=[0,1,2,3,4]\n",
    "    else:\n",
    "        choices=[0,1,2,3,4,' ']\n",
    "    for n in range(25):\n",
    "        ans=random.randrange(0,len(choices))\n",
    "        ansstring.append(str(choices[ans]))\n",
    "    ans=random.randrange(0,number_of_versions)\n",
    "    ansstring.append(str(ans))\n",
    "    ansstring=''.join(ansstring)\n",
    "    return ansstring\n",
    "def create_fakes(N, number_of_versions, blanks=False):\n",
    "    '''creates fakes and writes to an excel file'''\n",
    "    fakes_df=pd.DataFrame(index=range(N),columns = range(3),dtype=str)\n",
    "    fakes_df.iloc[:,0:2] =\"\"\n",
    "    for i in range(0,N):\n",
    "        fakes_df.iat[i,2]=create_afake(number_of_versions, blanks)\n",
    "    writer = pd.ExcelWriter('fakes.xlsx')\n",
    "    fakes_df.to_excel(writer,'Sheet1')\n",
    "    writer.save()\n",
    "\n",
    "########################################################\n",
    "############### Checking for incorrect Srl Nos################\n",
    "\n",
    "def find_oks(row):\n",
    "    return row['Name'].find(row['First Name'].upper())!=-1\n",
    "def remove_ok_rows(df):\n",
    "    notnull=df.dropna()\n",
    "    okrows=notnull.loc[notnull.apply(find_oks, axis=1)]\n",
    "    oksremoved=pd.concat([df, okrows]).drop_duplicates(keep=False)\n",
    "    return oksremoved\n",
    "\n",
    "def check_serial_numbers():\n",
    "    df=pd.read_csv('BSGrades.csv', usecols=[\"Last Name\", \"First Name\", \"Serial Number Text Grade <Text>\"])\n",
    "    df_all=pd.read_excel('All.xlsx', header=None, parse_cols=1,names = [\"Serial Number Text Grade <Text>\", \"Name\"])\n",
    "\n",
    "    checker_left = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='left')\n",
    "    checker_right = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='right')\n",
    "    u1=checker_left[checker_left.isnull().any(axis=1)]\n",
    "    u2=checker_left[checker_left['Serial Number Text Grade <Text>'].duplicated(keep=False)]\n",
    "    u3=checker_right[checker_right.isnull().any(axis=1)]\n",
    "    error_rpt=pd.concat([u1, u2, u3], axis=0)\n",
    "    if not error_rpt.empty:\n",
    "        error_rpt=remove_ok_rows(error_rpt)\n",
    "    error_rpt[\"Last Name\"] = error_rpt[\"Last Name\"]+' '+error_rpt['First Name']\n",
    "    error_rpt.drop('First Name', axis=1,inplace=True)\n",
    "    error_rpt.rename(columns={'Serial Number Text Grade <Text>': 'Srl No', 'Last Name':'Registered Name', 'Name':'Entered Name'}, inplace=True) \n",
    "    return error_rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually convert the dat file into an excel file using excel. Only extract the serial number,\n",
    "name and Answers, and make sure Answers is text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1009</td>\n",
       "      <td>HAN KARIN</td>\n",
       "      <td>0110241 21 4040</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Srl No       Name          Answers\n",
       "2   1009  HAN KARIN  0110241 21 4040"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Registered Name</th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Entered Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bolling Atiyaa</td>\n",
       "      <td>1003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Muezzinoglu Mine</td>\n",
       "      <td>1018</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Carlson Brynn</td>\n",
       "      <td>1002</td>\n",
       "      <td>MUEZZINOGLU MINE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Registered Name  Srl No      Entered Name\n",
       "1     Bolling Atiyaa    1003               NaN\n",
       "12  Muezzinoglu Mine    1018               NaN\n",
       "3      Carlson Brynn    1002  MUEZZINOGLU MINE"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#COLLECTING DATA\n",
    "rawdatafilename='All'\n",
    "xls_file = pd.ExcelFile(rawdatafilename+'.xlsx')\n",
    "df = xls_file.parse('Sheet1', header=None, parse_cols=2,names = [\"Srl No\", \"Name\", \"Answers\"], dtype='str')\n",
    "#parse_cols makes sure that only cols 0,1 and 2 are extracted\n",
    "#checking for blanks, print only if blanks found\n",
    "if not df[df['Answers'].str.contains(\" \")].empty:\n",
    "    blankers=df[df['Answers'].str.contains(\" \")]\n",
    "    display(blankers)\n",
    "ER=check_serial_numbers()\n",
    "if not ER.empty:\n",
    "    display(ER)\n",
    "df[\"Missed\"] = \"\"\n",
    "df[\"Score\"]=0\n",
    "df[\"Percentage\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing everything\n",
    "keydictionary={\"A\":\"0\",\"B\":\"1\",\"C\":\"2\",\"D\":\"3\",\"E\":\"4\"}\n",
    "invkeydictionary={0:\"A\",1:\"B\",2:\"C\",3:\"D\",4:\"E\"}\n",
    "points_per_question=3\n",
    "point_drop=2\n",
    "outs = getAllKeys(points_per_question)\n",
    "keylist=outs[\"keylist\"]\n",
    "pointlist=outs[\"pointlist\"]\n",
    "numberOfQuestions=outs[\"numberOfQuestions\"]\n",
    "if not keylist or not pointlist or numberOfQuestions==0:\n",
    "    sys.exit(\"Keys not properly imported\")\n",
    "totalpoints=sum(pointlist[2])\n",
    "new_totalpoints=totalpoints-point_drop\n",
    "numberOfVersions=len(keylist)\n",
    "numberOfStudents=df.shape[0]\n",
    "QIDs=createQuestionIDs(numberOfQuestions)\n",
    "headings=list(QIDs[0])#####headings=QIDs[0] assigns by reference, so changing headings will change QIDs[0]\n",
    "allAnswers_df = pd.DataFrame(index=range(numberOfStudents), columns = headings)# stores all student answers for each question, questions are the columns\n",
    "#results_df = pd.DataFrame(index=headings, columns = [\"A\",\"B\",\"C\",\"D\",\"E\",\"Skipped\",\"Diff\",\"Disc\"])\n",
    "headings.append(\"score\")\n",
    "analysis_df  = pd.DataFrame(index=range(numberOfStudents),columns = headings)# stores whether answer was correct, questions are the columns, last column is the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Name</th>\n",
       "      <th>Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1002</td>\n",
       "      <td>MUEZZINOGLU MINE</td>\n",
       "      <td>02022401103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1001</td>\n",
       "      <td>ACQUAYE AMBER</td>\n",
       "      <td>23100110212344</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Srl No              Name         Answers\n",
       "5    1002  MUEZZINOGLU MINE  02022401103400\n",
       "18   1001     ACQUAYE AMBER  23100110212344"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculations...\n",
      "Assuming version B for: MUEZZINOGLU MINE (Srl No: 1002).\n",
      "Assuming version A for: ACQUAYE AMBER (Srl No: 1001).\n",
      "Versions found:\n",
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "Done. That took: 0.1047203540802002 sec for 19 students\n",
      "Warning: Duplicates in serial numbers found!\n"
     ]
    }
   ],
   "source": [
    "##Everything initialized. Running the code now\n",
    "didnt_enter_versions=df[df['Answers'].map(len)!=numberOfQuestions]\n",
    "if not didnt_enter_versions.empty:\n",
    "    didnt_enter_versions=didnt_enter_versions[['Srl No', 'Name','Answers']]\n",
    "    display(didnt_enter_versions)\n",
    "print(\"Starting calculations...\")\n",
    "starttime = time.time()\n",
    "df=process_grades(df,outs, QIDs, analysis=True)\n",
    "number_correct_by_version=analyse_items(allAnswers_df, QIDs)\n",
    "item_analysis_df = make_item_analysis(analysis_df) \n",
    "endtime = time.time()\n",
    "print(\"Done. That took: \"+str(endtime-starttime)+ \" sec for \"+str(numberOfStudents)+\" students\")\n",
    "#check for duplicated serial numbers\n",
    "if not df[df['Srl No'].duplicated(keep=False)].empty:\n",
    "    print('Warning: Duplicates in serial numbers found!')\n",
    "    dupes=df[df['Srl No'].duplicated(keep=False)]\n",
    "max_score=analysis_df.at[analysis_df['score'].idxmax(),'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_csv(df,rawdatafilename)\n",
    "write_to_webpage(df, analysis_df, item_analysis_df, number_correct_by_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Grades and displays the N'th entry in the list using a keylist\n",
    "N=random.randint(1,df.shape[0]-1)\n",
    "#for N in range(df.shape[0]-1):\n",
    "check1=gradeWithKeylist(df.iat[N,2], outs, QIDs, analysis=False)\n",
    "print(\"Row \"+ str(N)+\": \"+df.iat[N,1]+\", \"+str(df.iat[N,0])+\". Missed \"+str(check1['missed'])+ \". Scored \"+str(check1['score'])+\"/\"+str(totalpoints))\n",
    "print('That is: '+str(100*check1['score']/new_totalpoints)+'% with '+str(point_drop)+' points dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "didnt_enter_versions=didnt_enter_versions[['Srl No', 'Name','Answers']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "didnt_enter_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
