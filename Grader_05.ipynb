{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import os.path\n",
    "import random\n",
    "from time import time\n",
    "import string\n",
    "import webbrowser\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import re\n",
    "from itertools import combinations\n",
    "from sys import exit\n",
    "\n",
    "def text_between_subtrings(s, start, end):\n",
    "    return s.find(start), s.find(end)\n",
    "\n",
    "def wordify(question, max_words=30):\n",
    "    question_words=question.split()\n",
    "    if len(question_words)>max_words:\n",
    "        question_words=question_words[:max_words]\n",
    "    q=' '.join(question_words)\n",
    "    #q=''.join(\"0\" if c.isdigit() else c for c in q)\n",
    "    q=re.sub('\\d+', '#', q)#convert multiple digits to single digits\n",
    "    q=re.sub('#.#', '#', q)#convert multiple digits to single digits\n",
    "    q=re.sub(' to power of ', '^', q)\n",
    "    q=re.sub('_+', '', q)\n",
    "    q=re.sub('\\d+\\)', '', q)#these two lines get rid of any 3) _______ crap\n",
    "    q = re.sub('#\\)', '', q)\n",
    "    if \"version is this\" in q:\n",
    "        q = re.sub('[\\bA-E]', 'V', q)  \n",
    "        q = re.sub('#\\)', '', q)\n",
    "    else:\n",
    "        \"Make sure the version question reads: What version is this test\"\n",
    "    return q.strip()\n",
    "\n",
    "def wordify_answers(answer, max_words=30):\n",
    "    answer_words=answer.split()\n",
    "    if len(answer_words)>max_words:\n",
    "        answer_words=answer_words[:max_words]\n",
    "    q=' '.join(answer_words)\n",
    "    q=re.sub(' to power of ', '^', q)\n",
    "    if len(answer_words)>1:\n",
    "        q=re.sub('[\\bA-E]\\)', '', q)#strips the A) for all except the version question\n",
    "    return q.strip()\n",
    "\n",
    "def extract_questions(s, max_words=10):\n",
    "    more='yes'\n",
    "    n=1\n",
    "    questionlist=[]\n",
    "    while more=='yes':\n",
    "        start = '\\n'+str(n)\n",
    "        end = '\\n'+str(n+1)\n",
    "        p1, p2 = text_between_subtrings(s, start, end)\n",
    "        if p1!=-1 and p2!=-1:\n",
    "            txt=s[p1+len(start):p2]\n",
    "            ###Grab the part between ) and nA, and convert into words\n",
    "            pp1, pp2 = text_between_subtrings(txt, ')', '\\nA')\n",
    "            txt=txt[1:txt.find('\\nA')]\n",
    "            txt=wordify(txt,max_words)\n",
    "            questionlist.append(txt)\n",
    "            n+=1\n",
    "        elif p1!=-1 and p2==-1:\n",
    "            txt=s[p1+len(start):]\n",
    "            pp1, pp2 = text_between_subtrings(txt, ')', '\\nA')\n",
    "            txt=txt[1:txt.find('\\nA')]\n",
    "            txt=wordify(txt,max_words)\n",
    "            #re.sub('[^A-Z]', '', s)\n",
    "            questionlist.append(txt)\n",
    "            more='no'\n",
    "        else:\n",
    "            print(\"Sum ting wong. Shouldn't go here. p1= \"+str(p1)+\" p2= \"+str(p2))\n",
    "            more='no'\n",
    "    return questionlist\n",
    "    \n",
    "\n",
    "def check_uniqueness_of_QIDs(All_IDs):\n",
    "    rr=range(len(All_IDs))\n",
    "    combs=list(combinations(rr,2))\n",
    "    for comb in combs:\n",
    "        u=[x for x in All_IDs[comb[0]] if x not in All_IDs[comb[1]]]\n",
    "        if u:\n",
    "            print(\"Oops! Some question ID's are not unique: \"+str(comb[0])+' and '+str(comb[1]))\n",
    "            return 'n'\n",
    "    return 'y'\n",
    "\n",
    "def createQuestionIDs_and_dfs(max_words=10):\n",
    "    All_IDs=[]\n",
    "    dfs=[]\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        if (os.path.isfile(filename)):\n",
    "            with open(filename, encoding=\"utf-8\") as file:  \n",
    "                data = file.read()\n",
    "            p=data.rfind('\\n\\n\\n\\n1)')\n",
    "            if p!=-1: \n",
    "                questions=data[:p]\n",
    "            else:\n",
    "                print('Make sure questions and answers in key'+str(n)+' are separated by \\n\\n\\n\\n')\n",
    "                return []\n",
    "            if questions.find('version is this')==-1:\n",
    "                print('Make sure there is a version question with the words \"version is this\". ')\n",
    "                return []\n",
    "            \n",
    "            qids=extract_questions(questions, max_words)\n",
    "            qnum=len(qids)\n",
    "            np_qids=np.array(qids).reshape(qnum,1)\n",
    "            All_IDs.append(qids)\n",
    "            \n",
    "            \n",
    "            aa=extract_choices(questions, qnum)\n",
    "            np_aa=np.array(aa).reshape(qnum,len(aa[0]))\n",
    "            v1=np.concatenate((np_qids,np_aa),axis=1)\n",
    "            df=pd.DataFrame(v1,columns=[\"Question\",\"A\",\"B\",\"C\",\"D\",\"E\"],index=range(1,qnum+1))\n",
    "            df['Blank']=''\n",
    "            dfs.append(df)\n",
    "            \n",
    "            \n",
    "    if check_uniqueness_of_QIDs(All_IDs)=='y':\n",
    "        return {\"IDs\":All_IDs, \"dfs\":dfs}\n",
    "    else:\n",
    "        print(\"QIDs not unique.\")\n",
    "        return {\"IDs\":[], \"dfs\":[]}\n",
    "\n",
    "def extract_choices(s, qnum):\n",
    "    all_choices=[]\n",
    "    for n in range(1,numberOfQuestions+1):\n",
    "        txt=''\n",
    "        start = '\\n'+str(n)\n",
    "        end = '\\n'+str(n+1)\n",
    "        p1, p2 = text_between_subtrings(s, start, end)\n",
    "        if p1!=-1 and p2!=-1:\n",
    "            txt=s[p1+len(start):p2]\n",
    "        if p1!=-1 and p2==-1:\n",
    "            txt=s[p1+len(start):]\n",
    "        anslist=[]\n",
    "        for m in range(0,4):\n",
    "            start=invkeydictionary[m]+')'\n",
    "            end=invkeydictionary[m+1]+')'\n",
    "            #looking for answer choices as the text between A) and B) for example\n",
    "            p1=txt.rfind(start)\n",
    "            p2=txt.rfind(end)\n",
    "            if p2!=-1:\n",
    "                ans=txt[p1:p2].strip()\n",
    "            else:\n",
    "                ans=txt[p1:].strip()\n",
    "            #if p2 was not found, do txt[p1:] rather than txt[p1:-1] which would truncate the last letter\n",
    "            ans=wordify_answers(ans)\n",
    "            if ans:\n",
    "                anslist.append(ans)\n",
    "            else:\n",
    "                anslist.append('')\n",
    "        if p2!=-1:\n",
    "            ans=txt[p2:].strip()\n",
    "            #ans=re.sub(' to power of ', '^', ans)\n",
    "            ans=wordify_answers(ans)\n",
    "            anslist.append(ans)\n",
    "        else:\n",
    "            anslist.append('')\n",
    "        all_choices.append(anslist)\n",
    "    return all_choices\n",
    "\n",
    "\n",
    "def getAllKeys(fixed_points_per_question=1):\n",
    "    '''read all files named key0-key4, make them into strings and and return a dictionary\n",
    "    containing a list of keys and also the number of questions'''\n",
    "    keylist=[]\n",
    "    pointlist=[]\n",
    "    for n in range(0,6):\n",
    "        filename=\"key\"+str(n)+\".txt\"\n",
    "        if (os.path.isfile(filename)):\n",
    "            with open(filename, encoding=\"utf-8\") as file: \n",
    "                data = file.read()\n",
    "            u=re.search('\\n\\n\\n\\n+1\\)',data)\n",
    "            if u is not None:\n",
    "                answers=data[u.start():].strip()\n",
    "            else:\n",
    "                print('Make sure questions and answers in key'+str(n)+' are separated by \\n\\n\\n\\n')\n",
    "                return None\n",
    "\n",
    "            ukey=re.findall('[\\bA-E]',answers)\n",
    "            ukey=[str(ord(x)-65) for x in ukey]\n",
    "            key=''.join(ukey)\n",
    "            keylist.append(key)\n",
    "            \n",
    "            qnum=len(key)\n",
    "\n",
    "            #This assumes that the answerkey contains points EITHER in the user-supplied form 8) A /tab 3/n OR testgen-supplied form \"Points: 3\"\n",
    "            upoints=re.findall('\\t\\d+',answers)+re.findall(':\\s\\d+',answers)\n",
    "            if upoints:\n",
    "                _=''.join(upoints)\n",
    "                points=re.sub('[^ \\d+]',' ', _).split()\n",
    "                points=[int(x) for x in points ]\n",
    "                assert(len(points)==qnum)\n",
    "            else:\n",
    "                points=[fixed_points_per_question]*qnum\n",
    "            \n",
    "            points[-1]=0\n",
    "            pointlist.append(points)\n",
    "    \n",
    "    if not keylist:\n",
    "        print(\"Answerkeys not found. Make sure they are named key0.txt, key1.txt etc\")\n",
    "    return {\"keylist\":keylist,\"pointlist\":pointlist,\"numberOfQuestions\":qnum}\n",
    "\n",
    "def process_grades(data,outs, QIDs,analysis=False):\n",
    "    '''grades all exams using correct keys, writes questions missed and scores'''\n",
    "\n",
    "    for NN in range(0, data.shape[0]):\n",
    "        #print(\"NN: \"+str(NN))\n",
    "        ans=data.iat[NN,2]\n",
    "        if len(ans)==numberOfQuestions-1:\n",
    "            v=guess_the_version(ans)\n",
    "            ans=ans+str(v)\n",
    "            print(\"Assuming version \"+invkeydictionary[v]+ \" for: \"+data.iat[NN,1]+\" (Srl No: \"+data.iat[NN,0]+\").\")\n",
    "        check1=gradeWithKeylist(ans, outs, QIDs, analysis, NN)\n",
    "        data.iat[NN,3]=check1['missed']\n",
    "        data.iat[NN,4]=check1['score']\n",
    "        data.iat[NN,5]=100.0*float(check1['score'])/float(new_totalpoints)\n",
    "\n",
    "    if analysis:\n",
    "        #remove the column with the version numbers\n",
    "        ####TO DO: find the version column rather than assume its the last one?\n",
    "        analysis_df.drop(analysis_df.columns[analysis_df.shape[1]-2], axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "def gradeWithKeylist(ans,outs, QIDs, analysis=False, N=0):\n",
    "    '''multiple versions - find the correct key as indicated on the last question on the exam '''\n",
    "    keylist=outs[\"keylist\"]\n",
    "    pointlist=outs[\"pointlist\"]\n",
    "    numberOfQuestions=outs[\"numberOfQuestions\"] \n",
    "    \n",
    "    assert(keylist!=[])\n",
    "    assert (ans[-1:] in ['0','1','2','3','4'])\n",
    "    whichKey=int(ans[-1:])\n",
    "    \n",
    "    key=keylist[whichKey]\n",
    "    points=pointlist[whichKey]\n",
    "    assert len(key)==len(ans)\n",
    "    missed=\"\"\n",
    "    rejalt=[1]*numberOfQuestions\n",
    "    for n in range(0,len(key)-1):\n",
    "        if key[n]!=ans[n]:\n",
    "            #print(\"storing\")\n",
    "            missed+=str(n+1)+\", \"\n",
    "            rejalt[n]=0\n",
    "    if sum(rejalt)==numberOfQuestions:\n",
    "        missed=\"v\"+invkeydictionary[whichKey]+\": \"+\"ALL CORRECT\"\n",
    "    else:\n",
    "        missed=\"v\"+invkeydictionary[whichKey]+\": \"+missed[:len(missed)-2]\n",
    "    score=sum([i*j for i,j in zip(points,rejalt)])\n",
    "    \n",
    "\n",
    "    mydict1 = dict(zip(QIDs[whichKey],rejalt))\n",
    "    #sortedIDs=sorted(mydict1.keys())\n",
    "    sorted_rejalt=[mydict1[k] for k in QIDs[0]]####its sorted according to v0\n",
    "    \n",
    "\n",
    "    \n",
    "    if analysis:   \n",
    "        sorted_rejalt.append(score)\n",
    "        analysis_df.loc[N] = sorted_rejalt \n",
    "        \n",
    "        mydict2 = dict(zip(QIDs[whichKey],ans))  \n",
    "        #sortedIDs=sorted(mydict2.keys())\n",
    "        sorted_ans=[mydict2[k] for k in QIDs[0]]\n",
    "        allAnswers_df.loc[N] = sorted_ans\n",
    "    \n",
    "\n",
    "        \n",
    "    return {'missed':missed, 'score':score}\n",
    "\n",
    "\n",
    "def guess_the_version(no_version):\n",
    "    scores=[]\n",
    "    for n in range(numberOfVersions):\n",
    "        try_version=no_version+str(n)\n",
    "        check1=gradeWithKeylist(try_version, outs, QIDs, analysis=False)\n",
    "        scores.append(check1['score'])\n",
    "    return scores.index(max(scores))\n",
    "\n",
    "\n",
    "\n",
    "def make_into_percent(adf):\n",
    "    '''Takes a dataframe and converts into np array to perform math operations.\n",
    "    In this case it divides each element by the sum of the row and times by 100 to make it a percent.'''\n",
    "    cols=list(adf)\n",
    "    #adf=adf.applymap(float)\n",
    "    np_df=adf.values\n",
    "    np_df=100*np_df/np.sum(np_df, axis=1, keepdims=True)\n",
    "    return pd.DataFrame(np_df,columns=cols)\n",
    "\n",
    "\n",
    "def mark_correct_answers(pardf,n,keylist):\n",
    "    \n",
    "    key_to_use=keylist[n]\n",
    "    for m in range(0,pardf.shape[0]):\n",
    "        cor=int(key_to_use[m])\n",
    "        pardf.iloc[m,cor]='[Correct] '+pardf.iloc[m,cor]\n",
    "    return pardf\n",
    "\n",
    "def analyse_items(a_df, koschen_paper, keylist):\n",
    "    '''create separate dfs with how many marked correct for each version separately'''\n",
    "    vers=['0','1','2','3','4']\n",
    "    lvers=['A','B','C','D','E']\n",
    "    outvars=[]\n",
    "    versionsfound=[]\n",
    "    students_in_this_version=[]\n",
    "\n",
    "    for n in range(0,len(koschen_paper)):\n",
    "        ch=str(n)\n",
    "        ##check for \"which version...\" to find the version and see if it matches ch\n",
    "        ver_qnum=koschen_paper[n].index[koschen_paper[n]['Question'].str.contains(\"version is this\")].tolist()[0]-1\n",
    "        #return the indexes of the cells which contain the given text, as a list\n",
    "        ver_q=koschen_paper[n].iloc[ver_qnum,0]\n",
    "        part_df = a_df.loc[a_df[ver_q]==ch]\n",
    "        if not part_df.empty:\n",
    "            print(\"Analysing data for version: \"+lvers[n])\n",
    "            part_df=part_df.T\n",
    "            part_df=part_df.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "            #part_df=addStarsToCorrectChoices(part_df,keylist,n, QIDs)\n",
    "            if ' ' in list(part_df):\n",
    "                part_df.rename(columns={' ': 'Blank'}, inplace=True)   \n",
    "            for pp in vers:\n",
    "                if pp in list(part_df):\n",
    "                    part_df.rename(columns={pp: lvers[int(pp)]}, inplace=True)\n",
    "            ####add back missing columns and rearrange in alphabetical order\n",
    "            cols_present=list(part_df)\n",
    "            allcols=['A','B','C','D','E','Blank']\n",
    "            cols_missing=[x for x in allcols if x not in cols_present]\n",
    "            if cols_missing:\n",
    "                for x in cols_missing:\n",
    "                    part_df[x]=0.0\n",
    "            part_df = part_df[allcols]\n",
    "            part_df = part_df.reindex(koschen_paper[n][\"Question\"])#This rearranges the questions as per the correct version\n",
    "            num_students=part_df.iloc[0,:].sum()\n",
    "            part_df=make_into_percent(part_df)\n",
    "            part_df=part_df.applymap('{:,.0f}%'.format)\n",
    "\n",
    "            kp=koschen_paper[n].copy()\n",
    "            kp.set_index(\"Question\", inplace=True)\n",
    "            kp.index.name=None ###This prevents an extra fictitious row from showing up in the header row\n",
    "            indexlist=kp.index.tolist()\n",
    "            part_df.index = indexlist\n",
    "            part_df=kp+\"<:\"+part_df+\":>\"\n",
    "            part_df=part_df.applymap(lambda m: re.sub('<:0%:>', '', m) )\n",
    "            \n",
    "            part_df=mark_correct_answers(part_df,n,keylist)\n",
    "            \n",
    "            \n",
    "            outvars.append(part_df)\n",
    "            versionsfound.append(lvers[n])\n",
    "            students_in_this_version.append(num_students)\n",
    "    \n",
    "    \n",
    "#     Return two dict so that the html function knows which version each table belongs to, and how many students took that version    \n",
    "    return dict(zip(versionsfound,outvars)), dict(zip(versionsfound,students_in_this_version))\n",
    "\n",
    "\n",
    "    \n",
    "def make_item_analysis(a_df):\n",
    "    \n",
    "    item_analysis_df=pd.DataFrame(index=[\"Difficulty\",\"Discrimination\"],columns = list(analysis_df))\n",
    "    n=a_df.shape[0]\n",
    "    v=analysis_df.values#converts into np array\n",
    "    \n",
    "    \n",
    "    #Calculating Difficulty\n",
    "    u1=np.sum(v,axis=0)/n\n",
    "    item_analysis_df.loc[\"Difficulty\"]=u1[0:u1.shape[0]]\n",
    "    \n",
    "    #Calculating Discrimination\n",
    "    students_per_group=n//3\n",
    "    if students_per_group<2:\n",
    "        print(\"Need more students for discrimination analysis (at least 7).\")\n",
    "    else:\n",
    "        v=v[0:v.shape[0]-1,:]#exclude the bottom row which had the sums (total number correct for each q)\n",
    "        v=v[v[:,v.shape[1]-1].argsort()[::-1]]#sort descending by scores\n",
    "        #print(v)\n",
    "        u1=np.sum(v[0:students_per_group,:],axis=0)#top scorers\n",
    "        u2=np.sum(v[n-students_per_group : n,:],axis=0)#bottom scorers\n",
    "        disc=(u1-u2)/students_per_group\n",
    "        disc.shape\n",
    "        item_analysis_df.loc[\"Discrimination\"]=disc[0:disc.shape[0]]\n",
    "    \n",
    "    item_analysis_df.drop(\"score\", axis=1, inplace=True)\n",
    "    return item_analysis_df.T\n",
    "\n",
    "#################Writing data#############################\n",
    "def write_to_xl(adf, rfilename):\n",
    "    adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    writer = pd.ExcelWriter(rfilename+'_processed.xlsx')\n",
    "    adf.to_excel(writer,'Sheet1',index=False)\n",
    "    writer.save()\n",
    "    \n",
    "def write_to_csv(adf, rfilename):\n",
    "    filename = rfilename+'_processed.csv'\n",
    "    #adf=adf.rename(columns={'Srl No': 'Serial Number Text Grade <Text>'})#This facilitates the Vlookup later\n",
    "    adf.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "def write_to_webpage(j_df, ia_df, pointdrop, ncbv={}, sbv={},  histobin=11):\n",
    "    filename = rawdatafilename+'_summary.html'\n",
    "    f = open(filename,'w',encoding='utf8')\n",
    "    \n",
    "    max_score=max(j_df['Score'])\n",
    "    number_of_maxes=len([x for x in j_df['Score'] if x==max_score])\n",
    "    \n",
    "    uu=j_df['Percentage'].describe().to_frame()\n",
    "    uu.rename(columns={'Percentage': \"Value\"}, index={'count':'Number of Students','mean':'Mean(%)', 'std':'Standard Deviation', 'min':'Lowest(%)','25%':'25th percentile', '50%':'50th percentile', '75%':'75th percentile', 'max':\"Highest(%)\"},inplace=True)\n",
    "    uu.loc['Maximum Available(%) '] = [100*totalpoints/new_totalpoints]\n",
    "    uu.loc['Number of Top Scorers'] = number_of_maxes\n",
    "    uu.loc['Points dropped '] = pointdrop\n",
    "\n",
    "    pre=\"<h2>Summary Data:</h2>\"\n",
    "    summary_table=uu.to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"sumdat\")\n",
    "    summary_table=pre+summary_table\n",
    "    post=\"<p>Score Distribution:</p>\"\n",
    "    \n",
    "    figure = plt.figure()\n",
    "    df['Percentage'].plot(kind='hist', bins=10)\n",
    "    plt.xlim(xmax=100*totalpoints/new_totalpoints)\n",
    "    figure.savefig('histo.svg')\n",
    "    \n",
    "    pre=\"<h2>Score Distribution:</h2>\"\n",
    "    image_code='<img src=\"histo.svg\" alt=\"histogram\">'\n",
    "    image=pre+image_code\n",
    "\n",
    "    pre=\"<h2>Item Analysis:</h2>\"\n",
    "    item_analysis_table=ia_df.sort_values('Difficulty').to_html(float_format=lambda x: '%10.2f' % x).replace(\"dataframe\" ,\"customers\")\n",
    "    item_analysis_table=pre+item_analysis_table\n",
    "\n",
    "    missed_tables=\"\"\n",
    "    if ncbv:\n",
    "        for ver in ['A', 'B','C','D','E']:\n",
    "            if ver in ncbv.keys():\n",
    "                if sbv:\n",
    "                    nn=str(int(sbv[ver]))\n",
    "                    pre=\"<h2>Distractor Analysis for Version \"+ver+\": (\"+nn+\" Students) </h2>\"\n",
    "                else:\n",
    "                    pre=\"<h2>Distractor Analysis for Version \"+ver+\":</h2>\"\n",
    "                post=ncbv[ver].to_html()\n",
    "                post=post.replace(\"dataframe\" ,\"howmany\")\n",
    "                post=post.replace('<td>[Correct]','<td class=\"correct\">')\n",
    "                post=post.replace('&lt;:','<span class=\"answered_by\">')\n",
    "                post=post.replace(':&gt;','</span>')\n",
    "                missed_tables+=pre+post\n",
    "\n",
    "    ##insert a script to make the webpage latex compatible. Edit the final  html file as \\( \\latex\\code \\)\n",
    "    html_start = \"\"\"<html>\n",
    "    <head>\n",
    "     <link rel=\"stylesheet\" type=\"text/css\" href=\"mystyle.css\">\n",
    "    <script type=\"text/javascript\" async\n",
    "    src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML\" async>\n",
    "    </script>\n",
    "    </head>\n",
    "    <body>\n",
    "    <h1>Data Analysis</h1>\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    html_end='''</body>\n",
    "    </html>'''\n",
    "    message=html_start+summary_table+image+item_analysis_table+missed_tables+html_end\n",
    "    f.write(message)\n",
    "    f.close()\n",
    "\n",
    "    #Change path to reflect file location\n",
    "    webbrowser.open_new_tab(filename)\n",
    "\n",
    "###############################################\n",
    "\n",
    "############### Checking for incorrect Srl Nos################\n",
    "\n",
    "\n",
    "def check_serial_numbers(gradebook,datafile):\n",
    "    df=pd.read_csv(gradebook, usecols=[\"Last Name\", \"First Name\", \"Serial Number Text Grade <Text>\"])\n",
    "    df_all=pd.read_excel(datafile, header=None, parse_cols=1,names = [\"Serial Number Text Grade <Text>\", \"Name\"])\n",
    "\n",
    "    checker_left = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='left')\n",
    "    checker_right = pd.merge(df,df_all[['Serial Number Text Grade <Text>','Name']],on='Serial Number Text Grade <Text>', how='right')\n",
    "    u1=checker_left[checker_left.isnull().any(axis=1)]\n",
    "    u2=checker_left[checker_left['Serial Number Text Grade <Text>'].duplicated(keep=False)]\n",
    "    u3=checker_right[checker_right.isnull().any(axis=1)]\n",
    "    error_rpt=pd.concat([u1, u2, u3], axis=0)\n",
    "    error_rpt[\"Last Name\"] = error_rpt[\"Last Name\"]+' '+error_rpt['First Name']\n",
    "    error_rpt.drop('First Name', axis=1,inplace=True)\n",
    "    error_rpt.rename(columns={'Serial Number Text Grade <Text>': 'Srl No', 'Last Name':'Registered Name', 'Name':'Entered Name'}, inplace=True) \n",
    "    return error_rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually convert the dat file into an excel file using excel. Only extract the serial number,\n",
    "name and Answers, and make sure Answers is text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Registered Name</th>\n",
       "      <th>Srl No</th>\n",
       "      <th>Entered Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aburto Julio Luis</td>\n",
       "      <td>1003</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>Raman Rohith</td>\n",
       "      <td>1055</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>Zong zongzong</td>\n",
       "      <td>1133</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Registered Name  Srl No Entered Name\n",
       "1    Aburto Julio Luis  1003    NaN        \n",
       "91   Raman Rohith       1055    NaN        \n",
       "127  Zong zongzong      1133    NaN        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#COLLECTING DATA\n",
    "rawdatafilename='All'\n",
    "datafilename=rawdatafilename+'.xlsx'\n",
    "gradebook='BSGrades.csv'\n",
    "if not (os.path.isfile(datafilename)) or not (os.path.isfile(gradebook)):\n",
    "    print('Make sure All.xlsx and BSGrades.csv are ready first')\n",
    "    exit(\"Necessary files not found\")\n",
    "xls_file = pd.ExcelFile(datafilename)\n",
    "df = xls_file.parse('Sheet1', header=None, parse_cols=2,names = [\"Srl No\", \"Name\", \"Answers\"], dtype='str')\n",
    "#parse_cols makes sure that only cols 0,1 and 2 are extracted\n",
    "#checking for blanks, print only if blanks found\n",
    "if not df[df['Answers'].str.contains(\" \")].empty:\n",
    "    blankers=df[df['Answers'].str.contains(\" \")]\n",
    "    display(blankers)\n",
    "ER=check_serial_numbers(gradebook,datafilename)\n",
    "if not ER.empty:\n",
    "    display(ER)\n",
    "\n",
    "df[\"Missed\"] = \"\"\n",
    "df[\"Score\"]=0\n",
    "df[\"Percentage\"]=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initializing everything\n",
    "keydictionary={\"A\":\"0\",\"B\":\"1\",\"C\":\"2\",\"D\":\"3\",\"E\":\"4\"}\n",
    "invkeydictionary={0:\"A\",1:\"B\",2:\"C\",3:\"D\",4:\"E\"}\n",
    "points_per_question=3\n",
    "point_drop=2\n",
    "outs = getAllKeys(points_per_question)\n",
    "keylist=outs[\"keylist\"]\n",
    "pointlist=outs[\"pointlist\"]\n",
    "numberOfQuestions=outs[\"numberOfQuestions\"]\n",
    "totalpoints=sum(pointlist[2])\n",
    "new_totalpoints=totalpoints-point_drop\n",
    "numberOfVersions=len(keylist)\n",
    "numberOfStudents=df.shape[0]\n",
    "quids_dfs=createQuestionIDs_and_dfs(max_words=50)\n",
    "QIDs=quids_dfs[\"IDs\"]\n",
    "koschen_paper=quids_dfs['dfs']\n",
    "if not keylist or not pointlist or numberOfQuestions==0 or not QIDs or not koschen_paper:\n",
    "    exit(\"Something went wrong in importing the questions\")\n",
    "headings=QIDs[0].copy()#####headings=QIDs[0] assigns by reference, so changing headings will change QIDs[0]\n",
    "allAnswers_df = pd.DataFrame(index=range(numberOfStudents), columns = headings)# stores all student answers for each question, questions are the columns\n",
    "headings.append(\"score\")\n",
    "analysis_df  = pd.DataFrame(index=range(numberOfStudents),columns = headings)# stores whether answer was correct, questions are the columns, last column is the score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting calculations...\n",
      "Analysing data for version: A\n",
      "Analysing data for version: B\n",
      "Analysing data for version: C\n",
      "Analysing data for version: D\n",
      "Done. That took: 0.2772691249847412 sec for 126 students\n"
     ]
    }
   ],
   "source": [
    "##Everything initialized. Running the code now\n",
    "didnt_enter_versions=df[df['Answers'].map(len)!=numberOfQuestions]\n",
    "if not didnt_enter_versions.empty:\n",
    "    didnt_enter_versions=didnt_enter_versions[['Srl No', 'Name','Answers']]\n",
    "    display(didnt_enter_versions)\n",
    "print(\"Starting calculations...\")\n",
    "starttime = time()\n",
    "df=process_grades(df,outs, QIDs, analysis=True)\n",
    "number_correct_by_version, students_by_version = analyse_items(allAnswers_df, koschen_paper,keylist)\n",
    "item_analysis_df = make_item_analysis(analysis_df) \n",
    "endtime = time()\n",
    "print(\"Done. That took: \"+str(endtime-starttime)+ \" sec for \"+str(numberOfStudents)+\" students\")\n",
    "#check for duplicated serial numbers\n",
    "dupes=df[df['Srl No'].duplicated(keep=False)]\n",
    "if not dupes.empty:\n",
    "    print('Warning: Duplicates in serial numbers found!')\n",
    "    display(dupes)    \n",
    "max_score=analysis_df.at[analysis_df['score'].idxmax(),'score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "write_to_csv(df.sort_values('Score', ascending=False),rawdatafilename)\n",
    "if (os.path.isfile('mystyle.css')):\n",
    "    write_to_webpage(df, item_analysis_df, point_drop, number_correct_by_version, students_by_version)\n",
    "else:\n",
    "    print(\"Copy the css file over first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grades and displays the N'th entry in the list using a keylist\n",
    "N=random.randint(1,df.shape[0]-1)\n",
    "#for N in range(df.shape[0]-1):\n",
    "#N=13\n",
    "check1=gradeWithKeylist(df.iat[N,2], outs, QIDs, analysis=False)\n",
    "print(\"Row \"+ str(N)+\": \"+df.iat[N,1]+\", \"+str(df.iat[N,0])+\". Missed \"+str(check1['missed'])+ \". Scored \"+str(check1['score'])+\"/\"+str(totalpoints))\n",
    "print('That is: '+str(100*check1['score']/new_totalpoints)+'% with '+str(point_drop)+' points dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################### DEV AREA###########################################\n",
    "\n",
    "\n",
    "# filename=\"key\"+str(0)+\".txt\"\n",
    "# if (os.path.isfile(filename)):\n",
    "#     with open(filename, encoding=\"utf-8\") as file:  \n",
    "#         data = file.read()\n",
    "#     p=data.rfind('\\n\\n\\n\\n1)')\n",
    "# s=data[:p]\n",
    "\n",
    "# n=25\n",
    "# start = '\\n'+str(n)\n",
    "# end = '\\n'+str(n+1)\n",
    "# p1, p2 = text_between_subtrings(s, start, end)\n",
    "# if p1!=-1 and p2!=-1:\n",
    "#     txt=s[p1+len(start):p2]\n",
    "# if p1!=-1 and p2==-1:\n",
    "#     txt=s[p1+len(start):]\n",
    "\n",
    "# print(txt)\n",
    "# anslist=[]\n",
    "# for m in range(0,4):\n",
    "#     start=invkeydictionary[m]+')'\n",
    "#     end=invkeydictionary[m+1]+')'\n",
    "#     #looking for answer choices as the text between A) and B) for example\n",
    "#     p1=txt.rfind(start)\n",
    "#     p2=txt.rfind(end)\n",
    "#     if p2!=-1:\n",
    "#         ans=txt[p1:p2].strip()\n",
    "#     else:\n",
    "#         ans=txt[p1:].strip()\n",
    "#     #if p2 was not found, do txt[p1:] rather than txt[p1:-1] which would truncate the last letter\n",
    "#     ans=wordify_answers(ans)\n",
    "#     if ans:\n",
    "#         print('p1= '+str(p1)+', p2= '+str(p2)+\" ans=\"+ans+' z1')\n",
    "#         anslist.append(ans)\n",
    "#     else:\n",
    "#         print('p1= '+str(p1)+', p2= '+str(p2)+\" ans= None\"+' z1')\n",
    "#         anslist.append('')\n",
    "# if p2!=-1:\n",
    "#     ans=txt[p2:].strip()\n",
    "#     #ans=re.sub(' to power of ', '^', ans)\n",
    "#     ans=wordify_answers(ans)\n",
    "#     print('p1= '+str(p1)+', p2= '+str(p2)+\" ans=\"+ans+' z2')\n",
    "#     anslist.append(ans)\n",
    "# else:\n",
    "#     print('p1= '+str(p1)+', p2= '+str(p2)+\" ans= None\"+' z3')\n",
    "#     anslist.append('')\n",
    "\n",
    "\n",
    "\n",
    "vers=['0','1','2','3','4']\n",
    "lvers=['A','B','C','D','E']\n",
    "n=2\n",
    "a_df=allAnswers_df\n",
    "ch=str(n)\n",
    "##check for \"which version...\" to find the version and see if it matches ch\n",
    "ver_qnum=koschen_paper[n].index[koschen_paper[n]['Question'].str.contains(\"version is this\")].tolist()[0]-1\n",
    "#return the indexes of the cells which contain the given text, as a list\n",
    "ver_q=koschen_paper[n].iloc[ver_qnum,0]\n",
    "part_df = a_df.loc[a_df[ver_q]==ch]\n",
    "if not part_df.empty:\n",
    "    print(\"Analysing data for version: \"+lvers[n])\n",
    "    part_df=part_df.T\n",
    "    part_df=part_df.apply(pd.Series.value_counts, axis=1).fillna(0)\n",
    "    #part_df=addStarsToCorrectChoices(part_df,keylist,n, QIDs)\n",
    "    if ' ' in list(part_df):\n",
    "        part_df.rename(columns={' ': 'Blank'}, inplace=True)   \n",
    "    for pp in vers:\n",
    "        if pp in list(part_df):\n",
    "            part_df.rename(columns={pp: lvers[int(pp)]}, inplace=True)\n",
    "    ####add back missing columns and rearrange in alphabetical order\n",
    "    cols_present=list(part_df)\n",
    "    allcols=['A','B','C','D','E','Blank']\n",
    "    cols_missing=[x for x in allcols if x not in cols_present]\n",
    "    if cols_missing:\n",
    "        for x in cols_missing:\n",
    "            part_df[x]=0.0\n",
    "    part_df = part_df[allcols]\n",
    "    part_df = part_df.reindex(koschen_paper[n][\"Question\"])\n",
    "    num_students_this_version=part_df.iloc[0,:].sum()\n",
    "    part_df=make_into_percent(part_df)\n",
    "    part_df=part_df.applymap('{:,.0f}%'.format)\n",
    "\n",
    "    kp=koschen_paper[n].copy()\n",
    "    kp.set_index(\"Question\", inplace=True)\n",
    "    kp.index.name=None ###This prevents an extra fictitious row from showing up in the header row\n",
    "    indexlist=kp.index.tolist()\n",
    "    part_df.index = indexlist\n",
    "    part_df=kp+\"<\"+part_df+\">\"\n",
    "    part_df=part_df.applymap(lambda m: re.sub('<0%>', '', m) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df.reindex(koschen_paper[2][\"Question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_df.reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post1=post.replace('<td>[Correct]','<td class=\"correct\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_score=max(df['Score'])\n",
    "number_of_maxes=len([x for x in df['Score'] if x==max_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i, x in enumerate(df['Score']) if x == 42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Srl No', 'Name', 'Answers', 'Missed', 'Score', 'Percentage']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############An attempt to develop a match finder\n",
    "\n",
    "# match_threshold=int(0.9*numberOfQuestions)\n",
    "# match_df=pd.DataFrame(columns=[\"Student 1\", \"Student 2\", \"Matches\"])\n",
    "\n",
    "# n=22\n",
    "# m=54\n",
    "# a1=df.loc[n,'Answers']\n",
    "# a2=df.loc[m, 'Answers']\n",
    "# matches=[ele for count,ele in enumerate(a1) if a2[count]==ele]\n",
    "# match_length=len(matches)\n",
    "# if match_length >= match_threshold:\n",
    "#     nn=len(match_df.index)\n",
    "#     match_df.loc[0, \"Student 1\"]=df.loc[n,'Name']+\" (\"+df.loc[n,'Srl No']+')'\n",
    "#     match_df.loc[0, \"Student 2\"]=df.loc[m,'Name']+\" (\"+df.loc[m,'Srl No']+')'\n",
    "#     match_df.loc[0, \"Matches\"]=matches\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
